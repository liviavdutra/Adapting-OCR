{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8e5b15",
   "metadata": {},
   "source": [
    "## Part 2 and Part 3\n",
    "\n",
    "All the changes are pointed trought comments with an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ab1b0b-e9b4-4106-9e73-30e12e58500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import six\n",
    "import random\n",
    "import lmdb\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "import logging\n",
    "\n",
    "import xml.etree.ElementTree as ET # import necessary due to the xml files\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from src.utils.utils import AverageMeter, Eval, OCRLabelConverter\n",
    "from src.utils.utils import EarlyStopping, gmkdir\n",
    "from src.optim.optimizer import STLR\n",
    "from src.utils.utils import gaussian\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21ea1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthDataset(Dataset):\n",
    "    def __init__(self, opt):\n",
    "        super(SynthDataset, self).__init__()\n",
    "        \n",
    "      \n",
    "        self.path = os.path.join(opt['path'], opt['imgdir'])\n",
    "     \n",
    "        self.images = os.listdir(self.path)\n",
    "\n",
    "        self.nSamples = len(self.images)\n",
    "        \n",
    "        f = lambda x: os.path.join(self.path, x)\n",
    "       \n",
    "        self.imagepaths = list(map(f, self.images))\n",
    "\n",
    "       \ttransform_list =  [transforms.Grayscale(1),\n",
    "                            transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.5,), (0.5,))]\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "        self.collate_fn = SynthCollator()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index <= len(self), 'index range error'\n",
    "        imagepath = self.imagepaths[index]\n",
    "        imagefile = os.path.basename(imagepath)\n",
    "        img = Image.open(imagepath)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        item = {'img': img, 'idx':index}\n",
    "        item['label'] = imagefile.split('_')[0]\n",
    "        return item \n",
    "    \n",
    "    #The function bellow was created to processes the image file of the car license plates using information from the XML annotation files. It extracts the paths from the image and annotation files, \n",
    "    #then iterates through the annotation files. For each annotation, it extracts the license plate identifier and searches for an image file with a matching identifier. \n",
    "    #If found, it returns early, ending the function without renaming any images, allowing the code to be run more times, even if the files have been changed already.\n",
    "    #If no match is found, it enters a loop to increment a counter, generating filenames, and  renaming the image files. \n",
    "    # I created this function to try use the ame logic applied in part1. I know that renaming the files, will alter the original dataset, so I kept always a copy of the original, if needed. \n",
    "   \n",
    "\n",
    "    @staticmethod\n",
    "    def get_label():\n",
    "\n",
    "        annotations = r'Car\\LP-characters\\annotations'\n",
    "        images = r'Car\\LP-characters\\images'\n",
    "        images_path = os.listdir(images)\n",
    "        annotations_path = os.listdir(annotations)\n",
    "        full_images_path = list(map(lambda x:os.path.join(images,x),images_path))\n",
    "        full_annotations_path = list(map(lambda x:os.path.join(annotations,x),annotations_path))\n",
    "        print(full_annotations_path)\n",
    "        print(full_images_path)\n",
    "        i = 0\n",
    "\n",
    "        for index,xml in enumerate(full_annotations_path): \n",
    "            tree = ET.parse(xml)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            objects = root.findall('object')\n",
    "            plate = ''\n",
    "            for element in objects: \n",
    "                for e in element : \n",
    "                    \n",
    "                    if e.tag == 'name':\n",
    "                        plate += e.text\n",
    "            \n",
    "            for name in images_path:\n",
    "                if plate in name: \n",
    "                    return\n",
    "\n",
    "            while True:\n",
    "                i+=1\n",
    "                try:\n",
    "                    os.rename(full_images_path[index],images+'\\\\'+plate+f'_{i}.png')\n",
    "                except:\n",
    "                    os.rename(full_images_path[index],images+'\\\\'+plate+f'_{i}.png')\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "# The function bellow was used to split the train and test dataset. It starts by setting a random seed for reproducibility, resizes the dataset's images using the function resize_images, \n",
    "# and defines directories for training and testing sets. If the training directory already contains images, the function returns early, because it considers the split is finished. \n",
    "# If not, it splits the dataset into training and testing sets b, copying  the images to the specified files.\n",
    "\n",
    "    @staticmethod\n",
    "    def train_test_split(test_split = .15):\n",
    "\n",
    "        torch.Generator().manual_seed(42)\n",
    "        SynthDataset.resize_images()\n",
    "        images = r'Car\\LP-characters\\images' \n",
    "        images_path = os.listdir(images) \n",
    "\n",
    "        os.makedirs(r'Car\\LP-characters\\images\\train',exist_ok=True)\n",
    "        os.makedirs(r'Car\\LP-characters\\images\\test',exist_ok=True)\n",
    "\n",
    "        train_images = r'Car\\LP-characters\\images\\train'\n",
    "        train_path = os.listdir(train_images)\n",
    "\n",
    "        if len(train_path)> 0:\n",
    "            return\n",
    "\n",
    "        train_list,test_list= random_split(images_path,[1-test_split,test_split])\n",
    "        \n",
    "        for c in train_list:\n",
    "            #print(c)\n",
    "            img = Image.open(images + '\\\\' + c)\n",
    "            img.save(r'Car\\LP-characters\\images\\train\\{}'.format(c)) \n",
    "        for c in test_list:\n",
    "            #print(c)\n",
    "            img = Image.open(images + '\\\\' + c)# abre a original\n",
    "            img.save(r'Car\\LP-characters\\images\\test\\{}'.format(c)) \n",
    "\n",
    "#  The padding used in the SynthCollator wasn't enough to adjust the images sizes. The function bellow is called in the previous function, and resizes the height and width of the images .\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_images(width=128,height=32):\n",
    "        images = r'Car\\LP-characters\\images'\n",
    "        images_path = os.listdir(images)\n",
    "\n",
    "        for c in images_path:\n",
    "            if not os.path.isfile('Car/LP-characters/images/'+c):\n",
    "                continue\n",
    "\n",
    "            img = Image.open('Car/LP-characters/images/'+c)\n",
    "            img = img.resize((img.width, height), Image.Resampling.LANCZOS)\n",
    "            img.save('Car/LP-characters/images/'+c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ad99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Car\\\\LP-characters\\\\annotations\\\\0000.xml', 'Car\\\\LP-characters\\\\annotations\\\\0001.xml', 'Car\\\\LP-characters\\\\annotations\\\\0003.xml', 'Car\\\\LP-characters\\\\annotations\\\\0004.xml', 'Car\\\\LP-characters\\\\annotations\\\\0005.xml', 'Car\\\\LP-characters\\\\annotations\\\\0006.xml', 'Car\\\\LP-characters\\\\annotations\\\\0008.xml', 'Car\\\\LP-characters\\\\annotations\\\\0009.xml', 'Car\\\\LP-characters\\\\annotations\\\\0010.xml', 'Car\\\\LP-characters\\\\annotations\\\\0011.xml', 'Car\\\\LP-characters\\\\annotations\\\\0012.xml', 'Car\\\\LP-characters\\\\annotations\\\\0013.xml', 'Car\\\\LP-characters\\\\annotations\\\\0014.xml', 'Car\\\\LP-characters\\\\annotations\\\\0015.xml', 'Car\\\\LP-characters\\\\annotations\\\\0016.xml', 'Car\\\\LP-characters\\\\annotations\\\\0017.xml', 'Car\\\\LP-characters\\\\annotations\\\\0018.xml', 'Car\\\\LP-characters\\\\annotations\\\\0019.xml', 'Car\\\\LP-characters\\\\annotations\\\\0020.xml', 'Car\\\\LP-characters\\\\annotations\\\\0021.xml', 'Car\\\\LP-characters\\\\annotations\\\\0022.xml', 'Car\\\\LP-characters\\\\annotations\\\\0023.xml', 'Car\\\\LP-characters\\\\annotations\\\\0024.xml', 'Car\\\\LP-characters\\\\annotations\\\\0025.xml', 'Car\\\\LP-characters\\\\annotations\\\\0028.xml', 'Car\\\\LP-characters\\\\annotations\\\\0029.xml', 'Car\\\\LP-characters\\\\annotations\\\\0030.xml', 'Car\\\\LP-characters\\\\annotations\\\\0031.xml', 'Car\\\\LP-characters\\\\annotations\\\\0032.xml', 'Car\\\\LP-characters\\\\annotations\\\\0033.xml', 'Car\\\\LP-characters\\\\annotations\\\\0034.xml', 'Car\\\\LP-characters\\\\annotations\\\\0035.xml', 'Car\\\\LP-characters\\\\annotations\\\\0037.xml', 'Car\\\\LP-characters\\\\annotations\\\\0038.xml', 'Car\\\\LP-characters\\\\annotations\\\\0039.xml', 'Car\\\\LP-characters\\\\annotations\\\\0040.xml', 'Car\\\\LP-characters\\\\annotations\\\\0041.xml', 'Car\\\\LP-characters\\\\annotations\\\\0042.xml', 'Car\\\\LP-characters\\\\annotations\\\\0043.xml', 'Car\\\\LP-characters\\\\annotations\\\\0044.xml', 'Car\\\\LP-characters\\\\annotations\\\\0045.xml', 'Car\\\\LP-characters\\\\annotations\\\\0046.xml', 'Car\\\\LP-characters\\\\annotations\\\\0047.xml', 'Car\\\\LP-characters\\\\annotations\\\\0048.xml', 'Car\\\\LP-characters\\\\annotations\\\\0050.xml', 'Car\\\\LP-characters\\\\annotations\\\\0051.xml', 'Car\\\\LP-characters\\\\annotations\\\\0052.xml', 'Car\\\\LP-characters\\\\annotations\\\\0053.xml', 'Car\\\\LP-characters\\\\annotations\\\\0054.xml', 'Car\\\\LP-characters\\\\annotations\\\\0055.xml', 'Car\\\\LP-characters\\\\annotations\\\\0057.xml', 'Car\\\\LP-characters\\\\annotations\\\\0058.xml', 'Car\\\\LP-characters\\\\annotations\\\\0059.xml', 'Car\\\\LP-characters\\\\annotations\\\\0060.xml', 'Car\\\\LP-characters\\\\annotations\\\\0061.xml', 'Car\\\\LP-characters\\\\annotations\\\\0062.xml', 'Car\\\\LP-characters\\\\annotations\\\\0063.xml', 'Car\\\\LP-characters\\\\annotations\\\\0065.xml', 'Car\\\\LP-characters\\\\annotations\\\\0066.xml', 'Car\\\\LP-characters\\\\annotations\\\\0067.xml', 'Car\\\\LP-characters\\\\annotations\\\\0068.xml', 'Car\\\\LP-characters\\\\annotations\\\\0069.xml', 'Car\\\\LP-characters\\\\annotations\\\\0070.xml', 'Car\\\\LP-characters\\\\annotations\\\\0071.xml', 'Car\\\\LP-characters\\\\annotations\\\\0072.xml', 'Car\\\\LP-characters\\\\annotations\\\\0073.xml', 'Car\\\\LP-characters\\\\annotations\\\\0074.xml', 'Car\\\\LP-characters\\\\annotations\\\\0076.xml', 'Car\\\\LP-characters\\\\annotations\\\\0077.xml', 'Car\\\\LP-characters\\\\annotations\\\\0078.xml', 'Car\\\\LP-characters\\\\annotations\\\\0079.xml', 'Car\\\\LP-characters\\\\annotations\\\\0080.xml', 'Car\\\\LP-characters\\\\annotations\\\\0081.xml', 'Car\\\\LP-characters\\\\annotations\\\\0082.xml', 'Car\\\\LP-characters\\\\annotations\\\\0083.xml', 'Car\\\\LP-characters\\\\annotations\\\\0084.xml', 'Car\\\\LP-characters\\\\annotations\\\\0085.xml', 'Car\\\\LP-characters\\\\annotations\\\\0086.xml', 'Car\\\\LP-characters\\\\annotations\\\\0087.xml', 'Car\\\\LP-characters\\\\annotations\\\\0088.xml', 'Car\\\\LP-characters\\\\annotations\\\\0089.xml', 'Car\\\\LP-characters\\\\annotations\\\\0091.xml', 'Car\\\\LP-characters\\\\annotations\\\\0092.xml', 'Car\\\\LP-characters\\\\annotations\\\\0093.xml', 'Car\\\\LP-characters\\\\annotations\\\\0094.xml', 'Car\\\\LP-characters\\\\annotations\\\\0095.xml', 'Car\\\\LP-characters\\\\annotations\\\\0096.xml', 'Car\\\\LP-characters\\\\annotations\\\\0097.xml', 'Car\\\\LP-characters\\\\annotations\\\\0098.xml', 'Car\\\\LP-characters\\\\annotations\\\\0099.xml', 'Car\\\\LP-characters\\\\annotations\\\\0100.xml', 'Car\\\\LP-characters\\\\annotations\\\\0101.xml', 'Car\\\\LP-characters\\\\annotations\\\\0102.xml', 'Car\\\\LP-characters\\\\annotations\\\\0103.xml', 'Car\\\\LP-characters\\\\annotations\\\\0105.xml', 'Car\\\\LP-characters\\\\annotations\\\\0106.xml', 'Car\\\\LP-characters\\\\annotations\\\\0108.xml', 'Car\\\\LP-characters\\\\annotations\\\\0109.xml', 'Car\\\\LP-characters\\\\annotations\\\\0110.xml', 'Car\\\\LP-characters\\\\annotations\\\\0111.xml', 'Car\\\\LP-characters\\\\annotations\\\\0112.xml', 'Car\\\\LP-characters\\\\annotations\\\\0113.xml', 'Car\\\\LP-characters\\\\annotations\\\\0115.xml', 'Car\\\\LP-characters\\\\annotations\\\\0116.xml', 'Car\\\\LP-characters\\\\annotations\\\\0117.xml', 'Car\\\\LP-characters\\\\annotations\\\\0118.xml', 'Car\\\\LP-characters\\\\annotations\\\\0119.xml', 'Car\\\\LP-characters\\\\annotations\\\\0120.xml', 'Car\\\\LP-characters\\\\annotations\\\\0121.xml', 'Car\\\\LP-characters\\\\annotations\\\\0122.xml', 'Car\\\\LP-characters\\\\annotations\\\\0123.xml', 'Car\\\\LP-characters\\\\annotations\\\\0125.xml', 'Car\\\\LP-characters\\\\annotations\\\\0127.xml', 'Car\\\\LP-characters\\\\annotations\\\\0128.xml', 'Car\\\\LP-characters\\\\annotations\\\\0129.xml', 'Car\\\\LP-characters\\\\annotations\\\\0130.xml', 'Car\\\\LP-characters\\\\annotations\\\\0131.xml', 'Car\\\\LP-characters\\\\annotations\\\\0132.xml', 'Car\\\\LP-characters\\\\annotations\\\\0133.xml', 'Car\\\\LP-characters\\\\annotations\\\\0134.xml', 'Car\\\\LP-characters\\\\annotations\\\\0135.xml', 'Car\\\\LP-characters\\\\annotations\\\\0136.xml', 'Car\\\\LP-characters\\\\annotations\\\\0137.xml', 'Car\\\\LP-characters\\\\annotations\\\\0138.xml', 'Car\\\\LP-characters\\\\annotations\\\\0139.xml', 'Car\\\\LP-characters\\\\annotations\\\\0140.xml', 'Car\\\\LP-characters\\\\annotations\\\\0141.xml', 'Car\\\\LP-characters\\\\annotations\\\\0142.xml', 'Car\\\\LP-characters\\\\annotations\\\\0143.xml', 'Car\\\\LP-characters\\\\annotations\\\\0144.xml', 'Car\\\\LP-characters\\\\annotations\\\\0145.xml', 'Car\\\\LP-characters\\\\annotations\\\\0146.xml', 'Car\\\\LP-characters\\\\annotations\\\\0147.xml', 'Car\\\\LP-characters\\\\annotations\\\\0148.xml', 'Car\\\\LP-characters\\\\annotations\\\\0149.xml', 'Car\\\\LP-characters\\\\annotations\\\\0150.xml', 'Car\\\\LP-characters\\\\annotations\\\\0151.xml', 'Car\\\\LP-characters\\\\annotations\\\\0152.xml', 'Car\\\\LP-characters\\\\annotations\\\\0154.xml', 'Car\\\\LP-characters\\\\annotations\\\\0155.xml', 'Car\\\\LP-characters\\\\annotations\\\\0156.xml', 'Car\\\\LP-characters\\\\annotations\\\\0159.xml', 'Car\\\\LP-characters\\\\annotations\\\\0160.xml', 'Car\\\\LP-characters\\\\annotations\\\\0161.xml', 'Car\\\\LP-characters\\\\annotations\\\\0162.xml', 'Car\\\\LP-characters\\\\annotations\\\\0163.xml', 'Car\\\\LP-characters\\\\annotations\\\\0164.xml', 'Car\\\\LP-characters\\\\annotations\\\\0167.xml', 'Car\\\\LP-characters\\\\annotations\\\\0168.xml', 'Car\\\\LP-characters\\\\annotations\\\\0169.xml', 'Car\\\\LP-characters\\\\annotations\\\\0170.xml', 'Car\\\\LP-characters\\\\annotations\\\\0171.xml', 'Car\\\\LP-characters\\\\annotations\\\\0172.xml', 'Car\\\\LP-characters\\\\annotations\\\\0173.xml', 'Car\\\\LP-characters\\\\annotations\\\\0174.xml', 'Car\\\\LP-characters\\\\annotations\\\\0176.xml', 'Car\\\\LP-characters\\\\annotations\\\\0177.xml', 'Car\\\\LP-characters\\\\annotations\\\\0178.xml', 'Car\\\\LP-characters\\\\annotations\\\\0179.xml', 'Car\\\\LP-characters\\\\annotations\\\\0181.xml', 'Car\\\\LP-characters\\\\annotations\\\\0183.xml', 'Car\\\\LP-characters\\\\annotations\\\\0184.xml', 'Car\\\\LP-characters\\\\annotations\\\\0185.xml', 'Car\\\\LP-characters\\\\annotations\\\\0186.xml', 'Car\\\\LP-characters\\\\annotations\\\\0187.xml', 'Car\\\\LP-characters\\\\annotations\\\\0188.xml', 'Car\\\\LP-characters\\\\annotations\\\\0189.xml', 'Car\\\\LP-characters\\\\annotations\\\\0190.xml', 'Car\\\\LP-characters\\\\annotations\\\\0191.xml', 'Car\\\\LP-characters\\\\annotations\\\\0192.xml', 'Car\\\\LP-characters\\\\annotations\\\\0193.xml', 'Car\\\\LP-characters\\\\annotations\\\\0195.xml', 'Car\\\\LP-characters\\\\annotations\\\\0198.xml', 'Car\\\\LP-characters\\\\annotations\\\\0199.xml', 'Car\\\\LP-characters\\\\annotations\\\\0201.xml', 'Car\\\\LP-characters\\\\annotations\\\\0202.xml', 'Car\\\\LP-characters\\\\annotations\\\\0203.xml', 'Car\\\\LP-characters\\\\annotations\\\\0204.xml', 'Car\\\\LP-characters\\\\annotations\\\\0205.xml', 'Car\\\\LP-characters\\\\annotations\\\\0206.xml', 'Car\\\\LP-characters\\\\annotations\\\\0207.xml', 'Car\\\\LP-characters\\\\annotations\\\\0208.xml', 'Car\\\\LP-characters\\\\annotations\\\\0209.xml', 'Car\\\\LP-characters\\\\annotations\\\\0210.xml', 'Car\\\\LP-characters\\\\annotations\\\\0211.xml', 'Car\\\\LP-characters\\\\annotations\\\\0212.xml', 'Car\\\\LP-characters\\\\annotations\\\\0213.xml', 'Car\\\\LP-characters\\\\annotations\\\\0214.xml', 'Car\\\\LP-characters\\\\annotations\\\\0215.xml', 'Car\\\\LP-characters\\\\annotations\\\\0216.xml', 'Car\\\\LP-characters\\\\annotations\\\\0217.xml', 'Car\\\\LP-characters\\\\annotations\\\\0218.xml', 'Car\\\\LP-characters\\\\annotations\\\\0219.xml', 'Car\\\\LP-characters\\\\annotations\\\\0220.xml', 'Car\\\\LP-characters\\\\annotations\\\\0221.xml', 'Car\\\\LP-characters\\\\annotations\\\\0222.xml', 'Car\\\\LP-characters\\\\annotations\\\\0223.xml', 'Car\\\\LP-characters\\\\annotations\\\\0224.xml', 'Car\\\\LP-characters\\\\annotations\\\\0226.xml', 'Car\\\\LP-characters\\\\annotations\\\\0227.xml', 'Car\\\\LP-characters\\\\annotations\\\\0228.xml', 'Car\\\\LP-characters\\\\annotations\\\\0229.xml', 'Car\\\\LP-characters\\\\annotations\\\\0230.xml', 'Car\\\\LP-characters\\\\annotations\\\\0231.xml', 'Car\\\\LP-characters\\\\annotations\\\\0232.xml', 'Car\\\\LP-characters\\\\annotations\\\\0233.xml', 'Car\\\\LP-characters\\\\annotations\\\\0234.xml', 'Car\\\\LP-characters\\\\annotations\\\\0235.xml', 'Car\\\\LP-characters\\\\annotations\\\\0236.xml']\n",
      "['Car\\\\LP-characters\\\\images\\\\0000.png', 'Car\\\\LP-characters\\\\images\\\\0001.png', 'Car\\\\LP-characters\\\\images\\\\0003.png', 'Car\\\\LP-characters\\\\images\\\\0004.png', 'Car\\\\LP-characters\\\\images\\\\0005.png', 'Car\\\\LP-characters\\\\images\\\\0006.png', 'Car\\\\LP-characters\\\\images\\\\0008.png', 'Car\\\\LP-characters\\\\images\\\\0009.png', 'Car\\\\LP-characters\\\\images\\\\0010.png', 'Car\\\\LP-characters\\\\images\\\\0011.png', 'Car\\\\LP-characters\\\\images\\\\0012.png', 'Car\\\\LP-characters\\\\images\\\\0013.png', 'Car\\\\LP-characters\\\\images\\\\0014.png', 'Car\\\\LP-characters\\\\images\\\\0015.png', 'Car\\\\LP-characters\\\\images\\\\0016.png', 'Car\\\\LP-characters\\\\images\\\\0017.png', 'Car\\\\LP-characters\\\\images\\\\0018.png', 'Car\\\\LP-characters\\\\images\\\\0019.png', 'Car\\\\LP-characters\\\\images\\\\0020.png', 'Car\\\\LP-characters\\\\images\\\\0021.png', 'Car\\\\LP-characters\\\\images\\\\0022.png', 'Car\\\\LP-characters\\\\images\\\\0023.png', 'Car\\\\LP-characters\\\\images\\\\0024.png', 'Car\\\\LP-characters\\\\images\\\\0025.png', 'Car\\\\LP-characters\\\\images\\\\0028.png', 'Car\\\\LP-characters\\\\images\\\\0029.png', 'Car\\\\LP-characters\\\\images\\\\0030.png', 'Car\\\\LP-characters\\\\images\\\\0031.png', 'Car\\\\LP-characters\\\\images\\\\0032.png', 'Car\\\\LP-characters\\\\images\\\\0033.png', 'Car\\\\LP-characters\\\\images\\\\0034.png', 'Car\\\\LP-characters\\\\images\\\\0035.png', 'Car\\\\LP-characters\\\\images\\\\0037.png', 'Car\\\\LP-characters\\\\images\\\\0038.png', 'Car\\\\LP-characters\\\\images\\\\0039.png', 'Car\\\\LP-characters\\\\images\\\\0040.png', 'Car\\\\LP-characters\\\\images\\\\0041.png', 'Car\\\\LP-characters\\\\images\\\\0042.png', 'Car\\\\LP-characters\\\\images\\\\0043.png', 'Car\\\\LP-characters\\\\images\\\\0044.png', 'Car\\\\LP-characters\\\\images\\\\0045.png', 'Car\\\\LP-characters\\\\images\\\\0046.png', 'Car\\\\LP-characters\\\\images\\\\0047.png', 'Car\\\\LP-characters\\\\images\\\\0048.png', 'Car\\\\LP-characters\\\\images\\\\0050.png', 'Car\\\\LP-characters\\\\images\\\\0051.png', 'Car\\\\LP-characters\\\\images\\\\0052.png', 'Car\\\\LP-characters\\\\images\\\\0053.png', 'Car\\\\LP-characters\\\\images\\\\0054.png', 'Car\\\\LP-characters\\\\images\\\\0055.png', 'Car\\\\LP-characters\\\\images\\\\0057.png', 'Car\\\\LP-characters\\\\images\\\\0058.png', 'Car\\\\LP-characters\\\\images\\\\0059.png', 'Car\\\\LP-characters\\\\images\\\\0060.png', 'Car\\\\LP-characters\\\\images\\\\0061.png', 'Car\\\\LP-characters\\\\images\\\\0062.png', 'Car\\\\LP-characters\\\\images\\\\0063.png', 'Car\\\\LP-characters\\\\images\\\\0065.png', 'Car\\\\LP-characters\\\\images\\\\0066.png', 'Car\\\\LP-characters\\\\images\\\\0067.png', 'Car\\\\LP-characters\\\\images\\\\0068.png', 'Car\\\\LP-characters\\\\images\\\\0069.png', 'Car\\\\LP-characters\\\\images\\\\0070.png', 'Car\\\\LP-characters\\\\images\\\\0071.png', 'Car\\\\LP-characters\\\\images\\\\0072.png', 'Car\\\\LP-characters\\\\images\\\\0073.png', 'Car\\\\LP-characters\\\\images\\\\0074.png', 'Car\\\\LP-characters\\\\images\\\\0076.png', 'Car\\\\LP-characters\\\\images\\\\0077.png', 'Car\\\\LP-characters\\\\images\\\\0078.png', 'Car\\\\LP-characters\\\\images\\\\0079.png', 'Car\\\\LP-characters\\\\images\\\\0080.png', 'Car\\\\LP-characters\\\\images\\\\0081.png', 'Car\\\\LP-characters\\\\images\\\\0082.png', 'Car\\\\LP-characters\\\\images\\\\0083.png', 'Car\\\\LP-characters\\\\images\\\\0084.png', 'Car\\\\LP-characters\\\\images\\\\0085.png', 'Car\\\\LP-characters\\\\images\\\\0086.png', 'Car\\\\LP-characters\\\\images\\\\0087.png', 'Car\\\\LP-characters\\\\images\\\\0088.png', 'Car\\\\LP-characters\\\\images\\\\0089.png', 'Car\\\\LP-characters\\\\images\\\\0091.png', 'Car\\\\LP-characters\\\\images\\\\0092.png', 'Car\\\\LP-characters\\\\images\\\\0093.png', 'Car\\\\LP-characters\\\\images\\\\0094.png', 'Car\\\\LP-characters\\\\images\\\\0095.png', 'Car\\\\LP-characters\\\\images\\\\0096.png', 'Car\\\\LP-characters\\\\images\\\\0097.png', 'Car\\\\LP-characters\\\\images\\\\0098.png', 'Car\\\\LP-characters\\\\images\\\\0099.png', 'Car\\\\LP-characters\\\\images\\\\0100.png', 'Car\\\\LP-characters\\\\images\\\\0101.png', 'Car\\\\LP-characters\\\\images\\\\0102.png', 'Car\\\\LP-characters\\\\images\\\\0103.png', 'Car\\\\LP-characters\\\\images\\\\0105.png', 'Car\\\\LP-characters\\\\images\\\\0106.png', 'Car\\\\LP-characters\\\\images\\\\0108.png', 'Car\\\\LP-characters\\\\images\\\\0109.png', 'Car\\\\LP-characters\\\\images\\\\0110.png', 'Car\\\\LP-characters\\\\images\\\\0111.png', 'Car\\\\LP-characters\\\\images\\\\0112.png', 'Car\\\\LP-characters\\\\images\\\\0113.png', 'Car\\\\LP-characters\\\\images\\\\0115.png', 'Car\\\\LP-characters\\\\images\\\\0116.png', 'Car\\\\LP-characters\\\\images\\\\0117.png', 'Car\\\\LP-characters\\\\images\\\\0118.png', 'Car\\\\LP-characters\\\\images\\\\0119.png', 'Car\\\\LP-characters\\\\images\\\\0120.png', 'Car\\\\LP-characters\\\\images\\\\0121.png', 'Car\\\\LP-characters\\\\images\\\\0122.png', 'Car\\\\LP-characters\\\\images\\\\0123.png', 'Car\\\\LP-characters\\\\images\\\\0125.png', 'Car\\\\LP-characters\\\\images\\\\0127.png', 'Car\\\\LP-characters\\\\images\\\\0128.png', 'Car\\\\LP-characters\\\\images\\\\0129.png', 'Car\\\\LP-characters\\\\images\\\\0130.png', 'Car\\\\LP-characters\\\\images\\\\0131.png', 'Car\\\\LP-characters\\\\images\\\\0132.png', 'Car\\\\LP-characters\\\\images\\\\0133.png', 'Car\\\\LP-characters\\\\images\\\\0134.png', 'Car\\\\LP-characters\\\\images\\\\0135.png', 'Car\\\\LP-characters\\\\images\\\\0136.png', 'Car\\\\LP-characters\\\\images\\\\0137.png', 'Car\\\\LP-characters\\\\images\\\\0138.png', 'Car\\\\LP-characters\\\\images\\\\0139.png', 'Car\\\\LP-characters\\\\images\\\\0140.png', 'Car\\\\LP-characters\\\\images\\\\0141.png', 'Car\\\\LP-characters\\\\images\\\\0142.png', 'Car\\\\LP-characters\\\\images\\\\0143.png', 'Car\\\\LP-characters\\\\images\\\\0144.png', 'Car\\\\LP-characters\\\\images\\\\0145.png', 'Car\\\\LP-characters\\\\images\\\\0146.png', 'Car\\\\LP-characters\\\\images\\\\0147.png', 'Car\\\\LP-characters\\\\images\\\\0148.png', 'Car\\\\LP-characters\\\\images\\\\0149.png', 'Car\\\\LP-characters\\\\images\\\\0150.png', 'Car\\\\LP-characters\\\\images\\\\0151.png', 'Car\\\\LP-characters\\\\images\\\\0152.png', 'Car\\\\LP-characters\\\\images\\\\0154.png', 'Car\\\\LP-characters\\\\images\\\\0155.png', 'Car\\\\LP-characters\\\\images\\\\0156.png', 'Car\\\\LP-characters\\\\images\\\\0159.png', 'Car\\\\LP-characters\\\\images\\\\0160.png', 'Car\\\\LP-characters\\\\images\\\\0161.png', 'Car\\\\LP-characters\\\\images\\\\0162.png', 'Car\\\\LP-characters\\\\images\\\\0163.png', 'Car\\\\LP-characters\\\\images\\\\0164.png', 'Car\\\\LP-characters\\\\images\\\\0167.png', 'Car\\\\LP-characters\\\\images\\\\0168.png', 'Car\\\\LP-characters\\\\images\\\\0169.png', 'Car\\\\LP-characters\\\\images\\\\0170.png', 'Car\\\\LP-characters\\\\images\\\\0171.png', 'Car\\\\LP-characters\\\\images\\\\0172.png', 'Car\\\\LP-characters\\\\images\\\\0173.png', 'Car\\\\LP-characters\\\\images\\\\0174.png', 'Car\\\\LP-characters\\\\images\\\\0176.png', 'Car\\\\LP-characters\\\\images\\\\0177.png', 'Car\\\\LP-characters\\\\images\\\\0178.png', 'Car\\\\LP-characters\\\\images\\\\0179.png', 'Car\\\\LP-characters\\\\images\\\\0181.png', 'Car\\\\LP-characters\\\\images\\\\0183.png', 'Car\\\\LP-characters\\\\images\\\\0184.png', 'Car\\\\LP-characters\\\\images\\\\0185.png', 'Car\\\\LP-characters\\\\images\\\\0186.png', 'Car\\\\LP-characters\\\\images\\\\0187.png', 'Car\\\\LP-characters\\\\images\\\\0188.png', 'Car\\\\LP-characters\\\\images\\\\0189.png', 'Car\\\\LP-characters\\\\images\\\\0190.png', 'Car\\\\LP-characters\\\\images\\\\0191.png', 'Car\\\\LP-characters\\\\images\\\\0192.png', 'Car\\\\LP-characters\\\\images\\\\0193.png', 'Car\\\\LP-characters\\\\images\\\\0195.png', 'Car\\\\LP-characters\\\\images\\\\0198.png', 'Car\\\\LP-characters\\\\images\\\\0199.png', 'Car\\\\LP-characters\\\\images\\\\0201.png', 'Car\\\\LP-characters\\\\images\\\\0202.png', 'Car\\\\LP-characters\\\\images\\\\0203.png', 'Car\\\\LP-characters\\\\images\\\\0204.png', 'Car\\\\LP-characters\\\\images\\\\0205.png', 'Car\\\\LP-characters\\\\images\\\\0206.png', 'Car\\\\LP-characters\\\\images\\\\0207.png', 'Car\\\\LP-characters\\\\images\\\\0208.png', 'Car\\\\LP-characters\\\\images\\\\0209.png', 'Car\\\\LP-characters\\\\images\\\\0210.png', 'Car\\\\LP-characters\\\\images\\\\0211.png', 'Car\\\\LP-characters\\\\images\\\\0212.png', 'Car\\\\LP-characters\\\\images\\\\0213.png', 'Car\\\\LP-characters\\\\images\\\\0214.png', 'Car\\\\LP-characters\\\\images\\\\0215.png', 'Car\\\\LP-characters\\\\images\\\\0216.png', 'Car\\\\LP-characters\\\\images\\\\0217.png', 'Car\\\\LP-characters\\\\images\\\\0218.png', 'Car\\\\LP-characters\\\\images\\\\0219.png', 'Car\\\\LP-characters\\\\images\\\\0220.png', 'Car\\\\LP-characters\\\\images\\\\0221.png', 'Car\\\\LP-characters\\\\images\\\\0222.png', 'Car\\\\LP-characters\\\\images\\\\0223.png', 'Car\\\\LP-characters\\\\images\\\\0224.png', 'Car\\\\LP-characters\\\\images\\\\0226.png', 'Car\\\\LP-characters\\\\images\\\\0227.png', 'Car\\\\LP-characters\\\\images\\\\0228.png', 'Car\\\\LP-characters\\\\images\\\\0229.png', 'Car\\\\LP-characters\\\\images\\\\0230.png', 'Car\\\\LP-characters\\\\images\\\\0231.png', 'Car\\\\LP-characters\\\\images\\\\0232.png', 'Car\\\\LP-characters\\\\images\\\\0233.png', 'Car\\\\LP-characters\\\\images\\\\0234.png', 'Car\\\\LP-characters\\\\images\\\\0235.png', 'Car\\\\LP-characters\\\\images\\\\0236.png']\n"
     ]
    }
   ],
   "source": [
    "SynthDataset.get_label()\n",
    "SynthDataset.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cd2af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcaac43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthCollator(object):\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        width = [item['img'].shape[2] for item in batch]\n",
    "        height  = [item['img'].shape[1] for item in batch]\n",
    "        #the height was created \n",
    "        indexes = [item['idx'] for item in batch]\n",
    "\n",
    "        #The tensor now contains the maximum height and width of the images in the batch. \n",
    "        #it was also necessary to state \"requires_grad=True\".\n",
    "        imgs = torch.ones([len(batch), batch[0]['img'].shape[0],max(height), \n",
    "                           max(width)], dtype=torch.float32,requires_grad=True)\n",
    "        for idx, item in enumerate(batch):\n",
    "            try:\n",
    "                #shape[1] was added.\n",
    "                imgs[idx, :,0:item['img'].shape[1], 0:item['img'].shape[2]] = item['img']\n",
    "                \n",
    "            except:\n",
    "                pass#print(imgs.shape)\n",
    "        item = {'img': imgs, 'idx':indexes}\n",
    "        if 'label' in batch[0].keys():\n",
    "            labels = [item['label'] for item in batch]\n",
    "            item['label'] = labels\n",
    "        return item\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6c20361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "    def forward(self, input):\n",
    "        self.rnn.flatten_parameters()\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, opt, leakyRelu=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert opt['imgH'] % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        #For part 5 the MaxPool12d was altered to AvgPool12d succefully increasing the accuracy (not much, but stll. It went from 9,12% to 11.06)\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = opt['nChannels'] if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)\n",
    "        #MaxPool12d changed\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.AvgPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "         #MaxPool12d changed\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.AvgPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "         #MaxPool12d changed\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.AvgPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.AvgPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "       \n",
    "        convRelu(6, True)  # 512x1x16\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential()\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(opt['nHidden']*2, opt['nHidden'], opt['nHidden']),\n",
    "            BidirectionalLSTM(opt['nHidden'], opt['nHidden'], opt['nClasses']))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        conv = self.cnn(input)\n",
    "        print(f\"Conv1 output shape: {conv.shape}\")\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        print(f\"Conv2 output shape: {conv.shape}\")\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        # rnn features\n",
    "        output = self.rnn(conv)\n",
    "        output = output.transpose(1,0) #Tbh to bth\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cfeede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCTCLoss(torch.nn.Module):\n",
    "    # T x B x H => Softmax on dimension 2\n",
    "    def __init__(self, dim=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
    "\n",
    "    def forward(self, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        EPS = 1e-7\n",
    "        loss = self.ctc_loss(logits, labels, prediction_sizes, target_sizes)\n",
    "        loss = self.sanitize(loss)\n",
    "        return self.debug(loss, logits, labels, prediction_sizes, target_sizes)\n",
    "    \n",
    "    def sanitize(self, loss):\n",
    "        EPS = 1e-7\n",
    "        if abs(loss.item() - float('inf')) < EPS:\n",
    "            return torch.zeros_like(loss)\n",
    "        if math.isnan(loss.item()):\n",
    "            return torch.zeros_like(loss)\n",
    "        return loss\n",
    "\n",
    "    def debug(self, loss, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Loss:\", loss)\n",
    "            print(\"logits:\", logits)\n",
    "            print(\"labels:\", labels)\n",
    "            print(\"prediction_sizes:\", prediction_sizes)\n",
    "            print(\"target_sizes:\", target_sizes)\n",
    "            raise Exception(\"NaN loss obtained. But why?\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca6dbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRTrainer(object):\n",
    "    def __init__(self, opt):\n",
    "        super(OCRTrainer, self).__init__()\n",
    "        self.data_train = opt['data_train']\n",
    "        self.data_val = opt['data_val']\n",
    "        self.model = opt['model']\n",
    "        self.criterion = opt['criterion']\n",
    "        self.optimizer = opt['optimizer']\n",
    "        self.schedule = opt['schedule']\n",
    "        self.converter = OCRLabelConverter(opt['alphabet'])\n",
    "        self.evaluator = Eval()\n",
    "        print('Scheduling is {}'.format(self.schedule))\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=opt['epochs'])\n",
    "        self.batch_size = opt['batch_size']\n",
    "        self.count = opt['epoch']\n",
    "        self.epochs = opt['epochs']\n",
    "        self.cuda = opt['cuda']\n",
    "        self.collate_fn = opt['collate_fn']\n",
    "        self.init_meters()\n",
    "\n",
    "    def init_meters(self):\n",
    "        self.avgTrainLoss = AverageMeter(\"Train loss\")\n",
    "        self.avgTrainCharAccuracy = AverageMeter(\"Train Character Accuracy\")\n",
    "        self.avgTrainWordAccuracy = AverageMeter(\"Train Word Accuracy\")\n",
    "        self.avgValLoss = AverageMeter(\"Validation loss\")\n",
    "        self.avgValCharAccuracy = AverageMeter(\"Validation Character Accuracy\")\n",
    "        self.avgValWordAccuracy = AverageMeter(\"Validation Word Accuracy\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits.transpose(1, 0)\n",
    "\n",
    "    def loss_fn(self, logits, targets, pred_sizes, target_sizes):\n",
    "        loss = self.criterion(logits, targets, pred_sizes, target_sizes)\n",
    "        return loss\n",
    "\n",
    "    def step(self):\n",
    "        self.max_grad_norm = 0.05\n",
    "        clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def schedule_lr(self):\n",
    "        if self.schedule:\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def _run_batch(self, batch, report_accuracy=False, validation=False):\n",
    "        input_, targets = batch['img'], batch['label']\n",
    "        targets, lengths = self.converter.encode(targets)\n",
    "        logits = self.forward(input_)\n",
    "        logits = logits.contiguous().cpu()\n",
    "        logits = torch.nn.functional.log_softmax(logits, 2)\n",
    "        T, B, H = logits.size()\n",
    "        pred_sizes = torch.LongTensor([T for i in range(B)])\n",
    "        targets= targets.view(-1).contiguous()\n",
    "        loss = self.loss_fn(logits, targets, pred_sizes, lengths)\n",
    "        if report_accuracy:\n",
    "            probs, preds = logits.max(2)\n",
    "            preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "            sim_preds = self.converter.decode(preds.data, pred_sizes.data, raw=False)\n",
    "            ca = np.mean((list(map(self.evaluator.char_accuracy, list(zip(sim_preds, batch['label']))))))\n",
    "            wa = np.mean((list(map(self.evaluator.word_accuracy, list(zip(sim_preds, batch['label']))))))\n",
    "        return loss, ca, wa\n",
    "\n",
    "    def run_epoch(self, validation=False):\n",
    "        if not validation:\n",
    "            loader = self.train_dataloader()\n",
    "            pbar = tqdm(loader, desc='Epoch: [%d]/[%d] Training'%(self.count, \n",
    "                self.epochs), leave=True)\n",
    "            self.model.train()\n",
    "        else:\n",
    "            loader = self.val_dataloader()\n",
    "            pbar = tqdm(loader, desc='Validating', leave=True)\n",
    "            self.model.eval()\n",
    "        outputs = []\n",
    "        for batch_nb, batch in enumerate(pbar):\n",
    "            if not validation:\n",
    "                output = self.training_step(batch)\n",
    "            else:\n",
    "                output = self.validation_step(batch)\n",
    "            pbar.set_postfix(output)\n",
    "            outputs.append(output)\n",
    "        self.schedule_lr()\n",
    "        if not validation:\n",
    "            result = self.train_end(outputs)\n",
    "        else:\n",
    "            result = self.validation_end(outputs)\n",
    "        return result\n",
    "\n",
    "# It was necessary to move the self.optimizer.zero_grad() to ensure that any gradients from previous iterations would not accumulate,and also set the variable\n",
    "# loss = torch.autograd.Variable(loss,requires_grad=True). Before doing this changes, the code woul crash with the error: element 0 of tensor doe not require grad and does not have a grad_fn\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        loss, ca, wa = self._run_batch(batch, report_accuracy=True)\n",
    "        loss = torch.autograd.Variable(loss,requires_grad=True)\n",
    "        loss.backward()\n",
    "        self.step()\n",
    "       \n",
    "        output = OrderedDict({\n",
    "            'loss': abs(loss.item()),\n",
    "            'train_ca': ca.item(),\n",
    "            'train_wa': wa.item()\n",
    "            })\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        loss, ca, wa = self._run_batch(batch, report_accuracy=True, validation=True)\n",
    "        output = OrderedDict({\n",
    "            'val_loss': abs(loss.item()),\n",
    "            'val_ca': ca.item(),\n",
    "            'val_wa': wa.item()\n",
    "            })\n",
    "        return output\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # logging.info('training data loader called')\n",
    "        loader = torch.utils.data.DataLoader(self.data_train,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.collate_fn,\n",
    "                shuffle=True)\n",
    "        return loader\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        # logging.info('val data loader called')\n",
    "        loader = torch.utils.data.DataLoader(self.data_val,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.collate_fn)\n",
    "        return loader\n",
    "\n",
    "    def train_end(self, outputs):\n",
    "        for output in outputs:\n",
    "            self.avgTrainLoss.add(output['loss'])\n",
    "            self.avgTrainCharAccuracy.add(output['train_ca'])\n",
    "            self.avgTrainWordAccuracy.add(output['train_wa'])\n",
    "\n",
    "        train_loss_mean = abs(self.avgTrainLoss.compute())\n",
    "        train_ca_mean = self.avgTrainCharAccuracy.compute()\n",
    "        train_wa_mean = self.avgTrainWordAccuracy.compute()\n",
    "\n",
    "        result = {'train_loss': train_loss_mean, 'train_ca': train_ca_mean,\n",
    "        'train_wa': train_wa_mean}\n",
    "        # result = {'progress_bar': tqdm_dict, 'log': tqdm_dict, 'val_loss': train_loss_mean}\n",
    "        return result\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        for output in outputs:\n",
    "            self.avgValLoss.add(output['val_loss'])\n",
    "            self.avgValCharAccuracy.add(output['val_ca'])\n",
    "            self.avgValWordAccuracy.add(output['val_wa'])\n",
    "\n",
    "        val_loss_mean = abs(self.avgValLoss.compute())\n",
    "        val_ca_mean = self.avgValCharAccuracy.compute()\n",
    "        val_wa_mean = self.avgValWordAccuracy.compute()\n",
    "\n",
    "        result = {'val_loss': val_loss_mean, 'val_ca': val_ca_mean,\n",
    "        'val_wa': val_wa_mean}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "282c4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    def __init__(self, model, optimizer, savepath=None, resume=False):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.savepath = os.path.join(savepath, 'best.ckpt')\n",
    "        self.cuda = torch.cuda.is_available() \n",
    "        self.cuda_count = torch.cuda.device_count()\n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        self.epoch = 0\n",
    "        if self.cuda_count > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.best_score = None\n",
    "        if resume and os.path.exists(self.savepath):\n",
    "            self.checkpoint = torch.load(self.savepath)\n",
    "            self.epoch = self.checkpoint['epoch']\n",
    "            self.best_score=self.checkpoint['best']\n",
    "            self.load()\n",
    "        else:\n",
    "            print('checkpoint does not exist')\n",
    "\n",
    "    def fit(self, opt):\n",
    "        opt['cuda'] = self.cuda\n",
    "        opt['model'] = self.model\n",
    "        opt['optimizer'] = self.optimizer\n",
    "        logging.basicConfig(filename=\"%s/%s.csv\" %(opt['log_dir'], opt['name']), level=logging.INFO)\n",
    "        self.saver = EarlyStopping(self.savepath, patience=15, verbose=True, best_score=self.best_score)\n",
    "        opt['epoch'] = self.epoch\n",
    "        trainer = OCRTrainer(opt)\n",
    "        \n",
    "        for epoch in range(opt['epoch'], opt['epochs']):\n",
    "            train_result = trainer.run_epoch()\n",
    "            val_result = trainer.run_epoch(validation=True)\n",
    "            trainer.count = epoch\n",
    "            info = '%d, %.6f, %.6f, %.6f, %.6f, %.6f, %.6f'%(epoch, train_result['train_loss'], \n",
    "                val_result['val_loss'], train_result['train_ca'],  val_result['val_ca'],\n",
    "                train_result['train_wa'], val_result['val_wa'])\n",
    "            logging.info(info)\n",
    "            self.val_loss = val_result['val_loss']\n",
    "            print(self.val_loss)\n",
    "            if self.savepath:\n",
    "                self.save(epoch)\n",
    "            if self.saver.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    def load(self):\n",
    "        print('Loading checkpoint at {} trained for {} epochs'.format(self.savepath, self.checkpoint['epoch']))\n",
    "        self.model.load_state_dict(self.checkpoint['state_dict'])\n",
    "        if 'opt_state_dict' in self.checkpoint.keys():\n",
    "            print('Loading optimizer')\n",
    "            self.optimizer.load_state_dict(self.checkpoint['opt_state_dict'])\n",
    "\n",
    "    def save(self, epoch):\n",
    "        self.saver(self.val_loss, epoch, self.model, self.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e28713af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traininig Data Size:142\n",
      "Val Data Size:36\n",
      "checkpoint does not exist\n",
      "None\n",
      "Scheduling is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:   0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:   3%|▎         | 1/29 [00:00<00:08,  3.21it/s, loss=27.5, train_ca=2.86, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 68])\n",
      "Conv2 output shape: torch.Size([5, 512, 68])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  14%|█▍        | 4/29 [00:00<00:03,  6.98it/s, loss=16.9, train_ca=0, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 104])\n",
      "Conv2 output shape: torch.Size([5, 512, 104])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 56])\n",
      "Conv2 output shape: torch.Size([5, 512, 56])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  21%|██        | 6/29 [00:01<00:04,  5.32it/s, loss=13, train_ca=2, train_wa=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 45])\n",
      "Conv2 output shape: torch.Size([5, 512, 45])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  31%|███       | 9/29 [00:01<00:03,  5.08it/s, loss=30, train_ca=2.5, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 88])\n",
      "Conv2 output shape: torch.Size([5, 512, 88])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  34%|███▍      | 10/29 [00:01<00:03,  5.65it/s, loss=31.2, train_ca=6, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 100])\n",
      "Conv2 output shape: torch.Size([5, 512, 100])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  45%|████▍     | 13/29 [00:02<00:02,  6.32it/s, loss=37.3, train_ca=8.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  52%|█████▏    | 15/29 [00:02<00:01,  7.34it/s, loss=36.1, train_ca=2, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 53])\n",
      "Conv2 output shape: torch.Size([5, 512, 53])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  55%|█████▌    | 16/29 [00:02<00:01,  7.83it/s, loss=24.9, train_ca=6.67, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 78])\n",
      "Conv2 output shape: torch.Size([5, 512, 78])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 187])\n",
      "Conv2 output shape: torch.Size([5, 512, 187])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  59%|█████▊    | 17/29 [00:02<00:01,  6.66it/s, loss=21.5, train_ca=2, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([5, 512, 71])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 138])\n",
      "Conv2 output shape: torch.Size([5, 512, 138])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  69%|██████▉   | 20/29 [00:03<00:01,  6.39it/s, loss=56.2, train_ca=4.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  72%|███████▏  | 21/29 [00:03<00:02,  3.81it/s, loss=179, train_ca=2, train_wa=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  83%|████████▎ | 24/29 [00:04<00:00,  6.03it/s, loss=14.3, train_ca=11, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 55])\n",
      "Conv2 output shape: torch.Size([5, 512, 55])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 47])\n",
      "Conv2 output shape: torch.Size([5, 512, 47])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  86%|████████▌ | 25/29 [00:04<00:01,  3.63it/s, loss=186, train_ca=8.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  93%|█████████▎| 27/29 [00:05<00:00,  4.34it/s, loss=33.1, train_ca=3.43, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 121])\n",
      "Conv2 output shape: torch.Size([5, 512, 121])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training: 100%|██████████| 29/29 [00:05<00:00,  5.35it/s, loss=34.1, train_ca=0, train_wa=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([2, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.86it/s, val_loss=11.3, val_ca=4, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  50%|█████     | 4/8 [00:00<00:00,  6.04it/s, val_loss=44.4, val_ca=2, val_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:00<00:00,  6.95it/s, val_loss=14.2, val_ca=0, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  6.96it/s, val_loss=14.6, val_ca=0, val_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "39.209441900253296\n",
      "Validation loss decreased (inf --> 39.209442).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:   0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 121])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:   7%|▋         | 2/29 [00:00<00:03,  7.77it/s, loss=22.7, train_ca=4, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conv2 output shape: torch.Size([5, 512, 121])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 70])\n",
      "Conv2 output shape: torch.Size([5, 512, 70])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  10%|█         | 3/29 [00:00<00:03,  8.62it/s, loss=19, train_ca=0, train_wa=0]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 64])\n",
      "Conv2 output shape: torch.Size([5, 512, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  17%|█▋        | 5/29 [00:00<00:02,  8.98it/s, loss=31.9, train_ca=2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 100])\n",
      "Conv2 output shape: torch.Size([5, 512, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  21%|██        | 6/29 [00:00<00:03,  7.04it/s, loss=58.2, train_ca=8.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 105])\n",
      "Conv2 output shape: torch.Size([5, 512, 105])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  31%|███       | 9/29 [00:01<00:02,  7.12it/s, loss=19.6, train_ca=13.2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 123])\n",
      "Conv2 output shape: torch.Size([5, 512, 123])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 61])\n",
      "Conv2 output shape: torch.Size([5, 512, 61])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  38%|███▊      | 11/29 [00:01<00:02,  6.95it/s, loss=38.3, train_ca=8.5, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  41%|████▏     | 12/29 [00:01<00:02,  7.13it/s, loss=23.9, train_ca=8.43, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  52%|█████▏    | 15/29 [00:02<00:01,  7.96it/s, loss=55.1, train_ca=2.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  55%|█████▌    | 16/29 [00:02<00:01,  7.55it/s, loss=21.8, train_ca=6.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 69])\n",
      "Conv2 output shape: torch.Size([5, 512, 69])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  62%|██████▏   | 18/29 [00:02<00:02,  4.99it/s, loss=23.7, train_ca=7.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([5, 512, 71])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  69%|██████▉   | 20/29 [00:03<00:01,  4.94it/s, loss=56, train_ca=6.22, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  72%|███████▏  | 21/29 [00:03<00:01,  4.47it/s, loss=27.5, train_ca=6, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 89])\n",
      "Conv2 output shape: torch.Size([5, 512, 89])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  83%|████████▎ | 24/29 [00:03<00:00,  5.88it/s, loss=19.8, train_ca=6.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 102])\n",
      "Conv2 output shape: torch.Size([5, 512, 102])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 66])\n",
      "Conv2 output shape: torch.Size([5, 512, 66])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  86%|████████▌ | 25/29 [00:04<00:01,  2.88it/s, loss=188, train_ca=8.44, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  90%|████████▉ | 26/29 [00:04<00:00,  3.51it/s, loss=22.9, train_ca=4, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 75])\n",
      "Conv2 output shape: torch.Size([5, 512, 75])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training:  93%|█████████▎| 27/29 [00:05<00:00,  2.40it/s, loss=179, train_ca=7.3, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0]/[15] Training: 100%|██████████| 29/29 [00:05<00:00,  5.11it/s, loss=21.4, train_ca=10, train_wa=0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 88])\n",
      "Conv2 output shape: torch.Size([5, 512, 88])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([2, 512, 71])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.40it/s, val_loss=11.3, val_ca=4, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:01,  4.99it/s, val_loss=33.9, val_ca=2.22, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:00<00:00,  5.81it/s, val_loss=24.9, val_ca=0, val_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:01<00:00,  5.81it/s, val_loss=14.2, val_ca=5.25, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  5.78it/s, val_loss=14.7, val_ca=0, val_wa=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "39.23762422800064\n",
      "EarlyStopping counter: (-39.209442 1 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:   0%|          | 0/29 [00:00<?, ?it/s, loss=21.5, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 64])\n",
      "Conv2 output shape: torch.Size([5, 512, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:   3%|▎         | 1/29 [00:00<00:03,  8.54it/s, loss=21.5, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  10%|█         | 3/29 [00:00<00:03,  6.63it/s, loss=25.4, train_ca=8, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 83])\n",
      "Conv2 output shape: torch.Size([5, 512, 83])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 102])\n",
      "Conv2 output shape: torch.Size([5, 512, 102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  17%|█▋        | 5/29 [00:00<00:03,  6.24it/s, loss=31.3, train_ca=2, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 100])\n",
      "Conv2 output shape: torch.Size([5, 512, 100])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  21%|██        | 6/29 [00:00<00:03,  5.94it/s, loss=37.9, train_ca=6.94, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 101])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  24%|██▍       | 7/29 [00:01<00:04,  5.42it/s, loss=33.1, train_ca=6, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  31%|███       | 9/29 [00:01<00:03,  5.65it/s, loss=21.2, train_ca=6.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 69])\n",
      "Conv2 output shape: torch.Size([5, 512, 69])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  38%|███▊      | 11/29 [00:01<00:02,  7.12it/s, loss=17.8, train_ca=4, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 56])\n",
      "Conv2 output shape: torch.Size([5, 512, 56])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 59])\n",
      "Conv2 output shape: torch.Size([5, 512, 59])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  41%|████▏     | 12/29 [00:01<00:02,  6.73it/s, loss=37.7, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  48%|████▊     | 14/29 [00:02<00:02,  5.97it/s, loss=23.3, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 75])\n",
      "Conv2 output shape: torch.Size([5, 512, 75])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  55%|█████▌    | 16/29 [00:02<00:02,  5.66it/s, loss=16.4, train_ca=13.1, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 187])\n",
      "Conv2 output shape: torch.Size([5, 512, 187])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 58])\n",
      "Conv2 output shape: torch.Size([5, 512, 58])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  59%|█████▊    | 17/29 [00:02<00:01,  6.31it/s, loss=26.1, train_ca=3.43, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 70])\n",
      "Conv2 output shape: torch.Size([5, 512, 70])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 138])\n",
      "Conv2 output shape: torch.Size([5, 512, 138])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  66%|██████▌   | 19/29 [00:03<00:03,  2.88it/s, loss=185, train_ca=2, train_wa=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  69%|██████▉   | 20/29 [00:04<00:04,  2.21it/s, loss=180, train_ca=2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  72%|███████▏  | 21/29 [00:04<00:02,  2.68it/s, loss=16.5, train_ca=2, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 56])\n",
      "Conv2 output shape: torch.Size([5, 512, 56])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  79%|███████▉  | 23/29 [00:04<00:01,  3.88it/s, loss=39.9, train_ca=2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 123])\n",
      "Conv2 output shape: torch.Size([5, 512, 123])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  86%|████████▌ | 25/29 [00:05<00:01,  3.50it/s, loss=107, train_ca=6.72, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  90%|████████▉ | 26/29 [00:05<00:00,  3.49it/s, loss=57.4, train_ca=4, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  93%|█████████▎| 27/29 [00:06<00:00,  2.71it/s, loss=101, train_ca=5.93, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training:  97%|█████████▋| 28/29 [00:06<00:00,  3.08it/s, loss=27.5, train_ca=6, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 89])\n",
      "Conv2 output shape: torch.Size([5, 512, 89])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 61])\n",
      "Conv2 output shape: torch.Size([2, 512, 61])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.30it/s, loss=19.2, train_ca=5.56, train_wa=0]\n",
      "Validating:  12%|█▎        | 1/8 [00:00<00:03,  2.01it/s, val_loss=11.4, val_ca=6, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:01,  4.46it/s, val_loss=33.9, val_ca=6.44, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:01<00:00,  5.42it/s, val_loss=24.9, val_ca=0, val_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 6/8 [00:01<00:00,  6.15it/s, val_loss=14.9, val_ca=9.47, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  5.09it/s, val_loss=14.6, val_ca=10, val_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "39.284395376841225\n",
      "EarlyStopping counter: (-39.209442 2 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:   3%|▎         | 1/29 [00:00<00:05,  4.79it/s, loss=35.5, train_ca=12.4, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 105])\n",
      "Conv2 output shape: torch.Size([5, 512, 105])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 100])\n",
      "Conv2 output shape: torch.Size([5, 512, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  10%|█         | 3/29 [00:00<00:08,  3.00it/s, loss=104, train_ca=8.44, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  14%|█▍        | 4/29 [00:01<00:07,  3.15it/s, loss=56.4, train_ca=2, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 104])\n",
      "Conv2 output shape: torch.Size([5, 512, 104])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  21%|██        | 6/29 [00:01<00:05,  4.47it/s, loss=23.7, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  24%|██▍       | 7/29 [00:01<00:04,  4.54it/s, loss=27.1, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 88])\n",
      "Conv2 output shape: torch.Size([5, 512, 88])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  31%|███       | 9/29 [00:02<00:07,  2.61it/s, loss=179, train_ca=6, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  38%|███▊      | 11/29 [00:02<00:04,  3.93it/s, loss=24.9, train_ca=8.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 76])\n",
      "Conv2 output shape: torch.Size([5, 512, 76])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  41%|████▏     | 12/29 [00:03<00:03,  4.73it/s, loss=17.1, train_ca=4, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 58])\n",
      "Conv2 output shape: torch.Size([5, 512, 58])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  45%|████▍     | 13/29 [00:03<00:03,  4.49it/s, loss=15.1, train_ca=1.43, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  52%|█████▏    | 15/29 [00:03<00:02,  5.98it/s, loss=18.9, train_ca=9.5, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 55])\n",
      "Conv2 output shape: torch.Size([5, 512, 55])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  55%|█████▌    | 16/29 [00:04<00:04,  3.24it/s, loss=185, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  59%|█████▊    | 17/29 [00:04<00:04,  2.76it/s, loss=119, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  62%|██████▏   | 18/29 [00:04<00:03,  3.23it/s, loss=39.3, train_ca=7, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  66%|██████▌   | 19/29 [00:05<00:03,  2.89it/s, loss=136, train_ca=4, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 67])\n",
      "Conv2 output shape: torch.Size([5, 512, 67])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  76%|███████▌  | 22/29 [00:05<00:01,  4.90it/s, loss=17.8, train_ca=2, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 70])\n",
      "Conv2 output shape: torch.Size([5, 512, 70])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  79%|███████▉  | 23/29 [00:06<00:01,  4.53it/s, loss=20.7, train_ca=4.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 187])\n",
      "Conv2 output shape: torch.Size([5, 512, 187])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 64])\n",
      "Conv2 output shape: torch.Size([5, 512, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  86%|████████▌ | 25/29 [00:06<00:00,  6.12it/s, loss=20.5, train_ca=2, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 50])\n",
      "Conv2 output shape: torch.Size([5, 512, 50])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 68])\n",
      "Conv2 output shape: torch.Size([5, 512, 68])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training:  93%|█████████▎| 27/29 [00:06<00:00,  6.22it/s, loss=47.7, train_ca=2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 138])\n",
      "Conv2 output shape: torch.Size([5, 512, 138])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [2]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.29it/s, loss=13.2, train_ca=0, train_wa=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 121])\n",
      "Conv2 output shape: torch.Size([5, 512, 121])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 38])\n",
      "Conv2 output shape: torch.Size([2, 512, 38])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.67it/s, val_loss=11.4, val_ca=4, val_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  5.41it/s, val_loss=33.9, val_ca=4.22, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:00<00:00,  6.31it/s, val_loss=15, val_ca=5.47, val_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  6.15it/s, val_loss=14.7, val_ca=10, val_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "39.31586417555809\n",
      "EarlyStopping counter: (-39.209442 3 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:   3%|▎         | 1/29 [00:00<00:05,  5.33it/s, loss=19, train_ca=0, train_wa=0]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 123])\n",
      "Conv2 output shape: torch.Size([5, 512, 123])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 64])\n",
      "Conv2 output shape: torch.Size([5, 512, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  14%|█▍        | 4/29 [00:00<00:03,  8.13it/s, loss=19, train_ca=10.7, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 66])\n",
      "Conv2 output shape: torch.Size([5, 512, 66])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 61])\n",
      "Conv2 output shape: torch.Size([5, 512, 61])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  21%|██        | 6/29 [00:00<00:02,  8.40it/s, loss=22, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 69])\n",
      "Conv2 output shape: torch.Size([5, 512, 69])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([5, 512, 71])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  28%|██▊       | 8/29 [00:00<00:02,  8.49it/s, loss=40.2, train_ca=2, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 52])\n",
      "Conv2 output shape: torch.Size([5, 512, 52])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  31%|███       | 9/29 [00:01<00:02,  7.49it/s, loss=36.1, train_ca=4.37, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 121])\n",
      "Conv2 output shape: torch.Size([5, 512, 121])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  34%|███▍      | 10/29 [00:01<00:05,  3.50it/s, loss=191, train_ca=4.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  38%|███▊      | 11/29 [00:02<00:04,  4.09it/s, loss=31.9, train_ca=2, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 100])\n",
      "Conv2 output shape: torch.Size([5, 512, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  45%|████▍     | 13/29 [00:02<00:03,  4.70it/s, loss=26, train_ca=2, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 76])\n",
      "Conv2 output shape: torch.Size([5, 512, 76])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  52%|█████▏    | 15/29 [00:02<00:02,  5.79it/s, loss=35.9, train_ca=9, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 48])\n",
      "Conv2 output shape: torch.Size([5, 512, 48])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 104])\n",
      "Conv2 output shape: torch.Size([5, 512, 104])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  59%|█████▊    | 17/29 [00:02<00:01,  6.32it/s, loss=36.2, train_ca=4, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 59])\n",
      "Conv2 output shape: torch.Size([5, 512, 59])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  62%|██████▏   | 18/29 [00:03<00:02,  5.09it/s, loss=16.9, train_ca=6.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 56])\n",
      "Conv2 output shape: torch.Size([5, 512, 56])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  69%|██████▉   | 20/29 [00:03<00:02,  4.39it/s, loss=117, train_ca=8.5, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  72%|███████▏  | 21/29 [00:04<00:02,  3.72it/s, loss=103, train_ca=2.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  83%|████████▎ | 24/29 [00:04<00:00,  5.29it/s, loss=40.8, train_ca=2.5, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 43])\n",
      "Conv2 output shape: torch.Size([5, 512, 43])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  86%|████████▌ | 25/29 [00:05<00:01,  3.24it/s, loss=182, train_ca=2.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training:  90%|████████▉ | 26/29 [00:05<00:00,  3.61it/s, loss=17.6, train_ca=16, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [3]/[15] Training: 100%|██████████| 29/29 [00:05<00:00,  4.92it/s, loss=39, train_ca=12.1, train_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 187])\n",
      "Conv2 output shape: torch.Size([5, 512, 187])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 102])\n",
      "Conv2 output shape: torch.Size([2, 512, 102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.61it/s, val_loss=11.3, val_ca=10, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  5.29it/s, val_loss=33.6, val_ca=10.2, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 6/8 [00:01<00:00,  6.76it/s, val_loss=14.8, val_ca=6.22, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  6.00it/s, val_loss=14.5, val_ca=10, val_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "39.28942379951477\n",
      "EarlyStopping counter: (-39.209442 4 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:   3%|▎         | 1/29 [00:00<00:02,  9.93it/s, loss=20.1, train_ca=8.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 66])\n",
      "Conv2 output shape: torch.Size([5, 512, 66])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:   7%|▋         | 2/29 [00:00<00:02,  9.07it/s, loss=20.6, train_ca=0, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 69])\n",
      "Conv2 output shape: torch.Size([5, 512, 69])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  10%|█         | 3/29 [00:00<00:04,  5.29it/s, loss=12.4, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 187])\n",
      "Conv2 output shape: torch.Size([5, 512, 187])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 43])\n",
      "Conv2 output shape: torch.Size([5, 512, 43])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  17%|█▋        | 5/29 [00:00<00:03,  6.75it/s, loss=31.7, train_ca=6.5, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  21%|██        | 6/29 [00:01<00:07,  3.15it/s, loss=215, train_ca=2, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  24%|██▍       | 7/29 [00:01<00:06,  3.50it/s, loss=32.7, train_ca=2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 138])\n",
      "Conv2 output shape: torch.Size([5, 512, 138])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 100])\n",
      "Conv2 output shape: torch.Size([5, 512, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  34%|███▍      | 10/29 [00:02<00:03,  5.68it/s, loss=13.9, train_ca=4.5, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 57])\n",
      "Conv2 output shape: torch.Size([5, 512, 57])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 45])\n",
      "Conv2 output shape: torch.Size([5, 512, 45])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  41%|████▏     | 12/29 [00:02<00:03,  5.51it/s, loss=15.4, train_ca=7, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 50])\n",
      "Conv2 output shape: torch.Size([5, 512, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  48%|████▊     | 14/29 [00:02<00:02,  7.15it/s, loss=18.8, train_ca=8, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 63])\n",
      "Conv2 output shape: torch.Size([5, 512, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  52%|█████▏    | 15/29 [00:03<00:03,  3.57it/s, loss=197, train_ca=2.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  55%|█████▌    | 16/29 [00:03<00:04,  3.12it/s, loss=110, train_ca=6, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  59%|█████▊    | 17/29 [00:03<00:03,  3.46it/s, loss=0, train_ca=0, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 123])\n",
      "Conv2 output shape: torch.Size([5, 512, 123])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 104])\n",
      "Conv2 output shape: torch.Size([5, 512, 104])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  66%|██████▌   | 19/29 [00:04<00:02,  3.93it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  69%|██████▉   | 20/29 [00:04<00:02,  3.13it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  79%|███████▉  | 23/29 [00:05<00:01,  4.71it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 88])\n",
      "Conv2 output shape: torch.Size([5, 512, 88])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 102])\n",
      "Conv2 output shape: torch.Size([5, 512, 102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  83%|████████▎ | 24/29 [00:05<00:00,  5.16it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  86%|████████▌ | 25/29 [00:05<00:01,  3.84it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training:  90%|████████▉ | 26/29 [00:06<00:00,  4.12it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 121])\n",
      "Conv2 output shape: torch.Size([5, 512, 121])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [4]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.49it/s, loss=0, train_ca=0, train_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([2, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.51it/s, val_loss=0, val_ca=0, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  5.46it/s, val_loss=0, val_ca=0, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 6/8 [00:01<00:00,  6.90it/s, val_loss=14.9, val_ca=5.47, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  6.04it/s, val_loss=14.7, val_ca=10, val_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "35.35856036345164\n",
      "Validation loss decreased (39.209442 --> 35.358560).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:   7%|▋         | 2/29 [00:00<00:03,  7.00it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 105])\n",
      "Conv2 output shape: torch.Size([5, 512, 105])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  10%|█         | 3/29 [00:00<00:03,  7.22it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  17%|█▋        | 5/29 [00:00<00:03,  6.35it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 68])\n",
      "Conv2 output shape: torch.Size([5, 512, 68])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  21%|██        | 6/29 [00:00<00:03,  7.18it/s, loss=13.9, train_ca=4.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 53])\n",
      "Conv2 output shape: torch.Size([5, 512, 53])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 47])\n",
      "Conv2 output shape: torch.Size([5, 512, 47])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  28%|██▊       | 8/29 [00:01<00:02,  8.11it/s, loss=16.5, train_ca=5.65, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 58])\n",
      "Conv2 output shape: torch.Size([5, 512, 58])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  34%|███▍      | 10/29 [00:02<00:05,  3.28it/s, loss=229, train_ca=4.72, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 53])\n",
      "Conv2 output shape: torch.Size([5, 512, 53])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  41%|████▏     | 12/29 [00:02<00:03,  4.73it/s, loss=41.7, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 53])\n",
      "Conv2 output shape: torch.Size([5, 512, 53])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 123])\n",
      "Conv2 output shape: torch.Size([5, 512, 123])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  48%|████▊     | 14/29 [00:03<00:05,  2.96it/s, loss=178, train_ca=2, train_wa=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  52%|█████▏    | 15/29 [00:03<00:04,  3.21it/s, loss=60.3, train_ca=2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  55%|█████▌    | 16/29 [00:03<00:03,  3.53it/s, loss=32.6, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 100])\n",
      "Conv2 output shape: torch.Size([5, 512, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  62%|██████▏   | 18/29 [00:03<00:02,  4.59it/s, loss=23.5, train_ca=2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  69%|██████▉   | 20/29 [00:04<00:01,  4.74it/s, loss=35.8, train_ca=2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  72%|███████▏  | 21/29 [00:04<00:01,  4.81it/s, loss=48.7, train_ca=11.4, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 138])\n",
      "Conv2 output shape: torch.Size([5, 512, 138])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  76%|███████▌  | 22/29 [00:04<00:01,  3.51it/s, loss=124, train_ca=11.4, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  83%|████████▎ | 24/29 [00:05<00:01,  4.73it/s, loss=22.4, train_ca=12.9, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 102])\n",
      "Conv2 output shape: torch.Size([5, 512, 102])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([5, 512, 71])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  90%|████████▉ | 26/29 [00:05<00:00,  5.81it/s, loss=25, train_ca=2, train_wa=0]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 70])\n",
      "Conv2 output shape: torch.Size([5, 512, 70])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 68])\n",
      "Conv2 output shape: torch.Size([5, 512, 68])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training:  93%|█████████▎| 27/29 [00:05<00:00,  4.08it/s, loss=102, train_ca=6.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.47it/s, loss=32.8, train_ca=16.2, train_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([2, 512, 93])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.59it/s, val_loss=11.3, val_ca=6, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  5.20it/s, val_loss=33.7, val_ca=6.44, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:00<00:00,  5.92it/s, val_loss=24.9, val_ca=0, val_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:01<00:00,  5.92it/s, val_loss=14.8, val_ca=6.22, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  5.64it/s, val_loss=14.5, val_ca=10, val_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "35.91171494552067\n",
      "EarlyStopping counter: (-35.358560 1 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:   3%|▎         | 1/29 [00:00<00:20,  1.38it/s, loss=187, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:   7%|▋         | 2/29 [00:01<00:18,  1.50it/s, loss=132, train_ca=8, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  10%|█         | 3/29 [00:02<00:21,  1.23it/s, loss=228, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  14%|█▍        | 4/29 [00:02<00:14,  1.70it/s, loss=38.8, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  21%|██        | 6/29 [00:02<00:07,  3.01it/s, loss=19.3, train_ca=6.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 63])\n",
      "Conv2 output shape: torch.Size([5, 512, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  24%|██▍       | 7/29 [00:03<00:08,  2.66it/s, loss=21.4, train_ca=5.43, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 75])\n",
      "Conv2 output shape: torch.Size([5, 512, 75])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  31%|███       | 9/29 [00:03<00:04,  4.20it/s, loss=17.8, train_ca=12.2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 138])\n",
      "Conv2 output shape: torch.Size([5, 512, 138])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  41%|████▏     | 12/29 [00:04<00:03,  5.64it/s, loss=19.2, train_ca=4, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 78])\n",
      "Conv2 output shape: torch.Size([5, 512, 78])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  45%|████▍     | 13/29 [00:04<00:02,  6.11it/s, loss=24.4, train_ca=2.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 73])\n",
      "Conv2 output shape: torch.Size([5, 512, 73])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  52%|█████▏    | 15/29 [00:04<00:02,  5.79it/s, loss=25.2, train_ca=4, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  55%|█████▌    | 16/29 [00:04<00:01,  6.57it/s, loss=20.1, train_ca=4.86, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 68])\n",
      "Conv2 output shape: torch.Size([5, 512, 68])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  59%|█████▊    | 17/29 [00:04<00:02,  5.65it/s, loss=59.7, train_ca=5.08, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  69%|██████▉   | 20/29 [00:05<00:01,  6.33it/s, loss=38, train_ca=9.22, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 64])\n",
      "Conv2 output shape: torch.Size([5, 512, 64])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 105])\n",
      "Conv2 output shape: torch.Size([5, 512, 105])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  76%|███████▌  | 22/29 [00:05<00:00,  7.72it/s, loss=16.2, train_ca=8.89, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 52])\n",
      "Conv2 output shape: torch.Size([5, 512, 52])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 50])\n",
      "Conv2 output shape: torch.Size([5, 512, 50])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  83%|████████▎ | 24/29 [00:05<00:00,  8.39it/s, loss=37.9, train_ca=8.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 58])\n",
      "Conv2 output shape: torch.Size([5, 512, 58])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  90%|████████▉ | 26/29 [00:06<00:00,  6.37it/s, loss=61.3, train_ca=2, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training:  93%|█████████▎| 27/29 [00:06<00:00,  4.09it/s, loss=97.9, train_ca=1.43, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 100])\n",
      "Conv2 output shape: torch.Size([5, 512, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [6]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.22it/s, loss=12.6, train_ca=0, train_wa=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([2, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([2, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.52it/s, val_loss=11.3, val_ca=4, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  5.13it/s, val_loss=33.9, val_ca=6.44, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:01<00:00,  5.86it/s, val_loss=14.8, val_ca=9.47, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  5.84it/s, val_loss=14.6, val_ca=10, val_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "36.342554569244385\n",
      "EarlyStopping counter: (-35.358560 2 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:   7%|▋         | 2/29 [00:00<00:03,  7.57it/s, loss=15.7, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 53])\n",
      "Conv2 output shape: torch.Size([5, 512, 53])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  10%|█         | 3/29 [00:00<00:03,  7.10it/s, loss=32.7, train_ca=4.5, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 100])\n",
      "Conv2 output shape: torch.Size([5, 512, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  14%|█▍        | 4/29 [00:00<00:04,  5.26it/s, loss=55.9, train_ca=2, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 102])\n",
      "Conv2 output shape: torch.Size([5, 512, 102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  21%|██        | 6/29 [00:01<00:04,  5.74it/s, loss=32, train_ca=4.86, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  28%|██▊       | 8/29 [00:01<00:07,  2.74it/s, loss=182, train_ca=8.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  31%|███       | 9/29 [00:02<00:05,  3.47it/s, loss=20, train_ca=1.43, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([5, 512, 71])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  34%|███▍      | 10/29 [00:02<00:06,  2.92it/s, loss=112, train_ca=4, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 89])\n",
      "Conv2 output shape: torch.Size([5, 512, 89])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  41%|████▏     | 12/29 [00:02<00:04,  3.66it/s, loss=72.1, train_ca=2.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 187])\n",
      "Conv2 output shape: torch.Size([5, 512, 187])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 104])\n",
      "Conv2 output shape: torch.Size([5, 512, 104])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  48%|████▊     | 14/29 [00:03<00:03,  4.92it/s, loss=42.2, train_ca=8.94, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([5, 512, 71])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 123])\n",
      "Conv2 output shape: torch.Size([5, 512, 123])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  52%|█████▏    | 15/29 [00:03<00:02,  5.07it/s, loss=15.3, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 52])\n",
      "Conv2 output shape: torch.Size([5, 512, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  59%|█████▊    | 17/29 [00:04<00:03,  3.35it/s, loss=177, train_ca=8.22, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  62%|██████▏   | 18/29 [00:04<00:03,  3.64it/s, loss=45.4, train_ca=4, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 138])\n",
      "Conv2 output shape: torch.Size([5, 512, 138])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  72%|███████▏  | 21/29 [00:04<00:01,  4.81it/s, loss=25, train_ca=2.22, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 105])\n",
      "Conv2 output shape: torch.Size([5, 512, 105])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 76])\n",
      "Conv2 output shape: torch.Size([5, 512, 76])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  76%|███████▌  | 22/29 [00:05<00:01,  5.10it/s, loss=36.9, train_ca=2, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  79%|███████▉  | 23/29 [00:05<00:01,  4.42it/s, loss=26.7, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 68])\n",
      "Conv2 output shape: torch.Size([5, 512, 68])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  86%|████████▌ | 25/29 [00:05<00:00,  5.21it/s, loss=16.6, train_ca=12.9, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 121])\n",
      "Conv2 output shape: torch.Size([5, 512, 121])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training:  93%|█████████▎| 27/29 [00:06<00:00,  4.59it/s, loss=27.3, train_ca=11.2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [7]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.49it/s, loss=25.5, train_ca=16.2, train_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([2, 512, 1, 74])\n",
      "Conv2 output shape: torch.Size([2, 512, 74])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:03,  2.31it/s, val_loss=11.3, val_ca=6, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  5.11it/s, val_loss=33.8, val_ca=6.44, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 6/8 [00:01<00:00,  6.99it/s, val_loss=14.9, val_ca=5.47, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  5.83it/s, val_loss=14.7, val_ca=10, val_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "36.685703939861725\n",
      "EarlyStopping counter: (-35.358560 3 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:   0%|          | 0/29 [00:00<?, ?it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 75])\n",
      "Conv2 output shape: torch.Size([5, 512, 75])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:   7%|▋         | 2/29 [00:00<00:04,  5.47it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  17%|█▋        | 5/29 [00:00<00:03,  7.02it/s, loss=20.8, train_ca=8, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 69])\n",
      "Conv2 output shape: torch.Size([5, 512, 69])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  24%|██▍       | 7/29 [00:01<00:04,  4.99it/s, loss=21.5, train_ca=2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([5, 512, 71])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  31%|███       | 9/29 [00:01<00:03,  5.24it/s, loss=16.1, train_ca=5, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 187])\n",
      "Conv2 output shape: torch.Size([5, 512, 187])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([5, 512, 51])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  34%|███▍      | 10/29 [00:01<00:03,  5.76it/s, loss=25.5, train_ca=6, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 83])\n",
      "Conv2 output shape: torch.Size([5, 512, 83])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  38%|███▊      | 11/29 [00:02<00:03,  4.72it/s, loss=57.9, train_ca=2.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  41%|████▏     | 12/29 [00:02<00:04,  3.52it/s, loss=26.7, train_ca=15, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  48%|████▊     | 14/29 [00:02<00:03,  4.58it/s, loss=18.7, train_ca=6, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 104])\n",
      "Conv2 output shape: torch.Size([5, 512, 104])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 63])\n",
      "Conv2 output shape: torch.Size([5, 512, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  55%|█████▌    | 16/29 [00:03<00:03,  3.30it/s, loss=191, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  59%|█████▊    | 17/29 [00:03<00:03,  3.84it/s, loss=25.7, train_ca=7.43, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 89])\n",
      "Conv2 output shape: torch.Size([5, 512, 89])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  62%|██████▏   | 18/29 [00:04<00:03,  3.27it/s, loss=107, train_ca=12.9, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  66%|██████▌   | 19/29 [00:04<00:02,  3.67it/s, loss=35.3, train_ca=14.2, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 121])\n",
      "Conv2 output shape: torch.Size([5, 512, 121])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  69%|██████▉   | 20/29 [00:05<00:03,  2.52it/s, loss=0, train_ca=0, train_wa=0]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 68])\n",
      "Conv2 output shape: torch.Size([5, 512, 68])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  79%|███████▉  | 23/29 [00:05<00:01,  4.36it/s, loss=34.7, train_ca=14.6, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 70])\n",
      "Conv2 output shape: torch.Size([5, 512, 70])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 102])\n",
      "Conv2 output shape: torch.Size([5, 512, 102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  86%|████████▌ | 25/29 [00:05<00:00,  5.73it/s, loss=20.5, train_ca=7.3, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 105])\n",
      "Conv2 output shape: torch.Size([5, 512, 105])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 59])\n",
      "Conv2 output shape: torch.Size([5, 512, 59])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training:  90%|████████▉ | 26/29 [00:06<00:00,  6.28it/s, loss=18.4, train_ca=4.5, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 66])\n",
      "Conv2 output shape: torch.Size([5, 512, 66])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [8]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.63it/s, loss=14.4, train_ca=0, train_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 52])\n",
      "Conv2 output shape: torch.Size([2, 512, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:03,  2.32it/s, val_loss=11.3, val_ca=10, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  5.19it/s, val_loss=33.6, val_ca=10.2, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:00<00:00,  5.83it/s, val_loss=25, val_ca=0, val_wa=0]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:01<00:00,  5.83it/s, val_loss=13.1, val_ca=5.47, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  5.72it/s, val_loss=14.7, val_ca=10, val_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "36.92318583726883\n",
      "EarlyStopping counter: (-35.358560 4 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:   0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 76])\n",
      "Conv2 output shape: torch.Size([5, 512, 76])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:   7%|▋         | 2/29 [00:00<00:05,  4.53it/s, loss=56.1, train_ca=10, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  10%|█         | 3/29 [00:00<00:04,  5.32it/s, loss=37, train_ca=2.5, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 64])\n",
      "Conv2 output shape: torch.Size([5, 512, 64])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  24%|██▍       | 7/29 [00:01<00:02,  7.60it/s, loss=26.1, train_ca=6.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 45])\n",
      "Conv2 output shape: torch.Size([5, 512, 45])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 83])\n",
      "Conv2 output shape: torch.Size([5, 512, 83])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  24%|██▍       | 7/29 [00:01<00:02,  7.60it/s, loss=20.3, train_ca=6.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 66])\n",
      "Conv2 output shape: torch.Size([5, 512, 66])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  31%|███       | 9/29 [00:01<00:02,  7.04it/s, loss=51.8, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 138])\n",
      "Conv2 output shape: torch.Size([5, 512, 138])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  38%|███▊      | 11/29 [00:01<00:03,  5.65it/s, loss=56.3, train_ca=4.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  41%|████▏     | 12/29 [00:02<00:05,  3.15it/s, loss=198, train_ca=6.72, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  45%|████▍     | 13/29 [00:02<00:04,  3.53it/s, loss=33.3, train_ca=6.71, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 121])\n",
      "Conv2 output shape: torch.Size([5, 512, 121])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  48%|████▊     | 14/29 [00:03<00:05,  2.96it/s, loss=119, train_ca=6.44, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  52%|█████▏    | 15/29 [00:03<00:03,  3.51it/s, loss=19.4, train_ca=8.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 88])\n",
      "Conv2 output shape: torch.Size([5, 512, 88])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 61])\n",
      "Conv2 output shape: torch.Size([5, 512, 61])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  59%|█████▊    | 17/29 [00:03<00:02,  4.95it/s, loss=34.1, train_ca=3.43, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 50])\n",
      "Conv2 output shape: torch.Size([5, 512, 50])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  69%|██████▉   | 20/29 [00:03<00:01,  6.22it/s, loss=21.4, train_ca=6, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 70])\n",
      "Conv2 output shape: torch.Size([5, 512, 70])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([5, 512, 71])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  72%|███████▏  | 21/29 [00:04<00:01,  6.15it/s, loss=37.2, train_ca=5, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 105])\n",
      "Conv2 output shape: torch.Size([5, 512, 105])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  76%|███████▌  | 22/29 [00:04<00:01,  4.10it/s, loss=112, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 89])\n",
      "Conv2 output shape: torch.Size([5, 512, 89])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  83%|████████▎ | 24/29 [00:04<00:00,  5.20it/s, loss=31, train_ca=6.5, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  86%|████████▌ | 25/29 [00:05<00:01,  2.78it/s, loss=197, train_ca=18.1, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training:  93%|█████████▎| 27/29 [00:05<00:00,  3.92it/s, loss=26.2, train_ca=7, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 102])\n",
      "Conv2 output shape: torch.Size([5, 512, 102])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [9]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.74it/s, loss=31.1, train_ca=5, train_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 68])\n",
      "Conv2 output shape: torch.Size([5, 512, 68])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 100])\n",
      "Conv2 output shape: torch.Size([2, 512, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.64it/s, val_loss=11.3, val_ca=6, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  5.43it/s, val_loss=33.7, val_ca=6.44, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:00<00:00,  6.09it/s, val_loss=25, val_ca=0, val_wa=0]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:01<00:00,  6.09it/s, val_loss=14.9, val_ca=5.47, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  5.87it/s, val_loss=14.7, val_ca=10, val_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "37.14613296768882\n",
      "EarlyStopping counter: (-35.358560 5 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:   3%|▎         | 1/29 [00:00<00:06,  4.04it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 121])\n",
      "Conv2 output shape: torch.Size([5, 512, 121])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  10%|█         | 3/29 [00:00<00:04,  5.87it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  14%|█▍        | 4/29 [00:01<00:09,  2.73it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  17%|█▋        | 5/29 [00:01<00:07,  3.18it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 138])\n",
      "Conv2 output shape: torch.Size([5, 512, 138])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  21%|██        | 6/29 [00:01<00:06,  3.73it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  28%|██▊       | 8/29 [00:01<00:04,  4.42it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  34%|███▍      | 10/29 [00:02<00:04,  4.30it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 67])\n",
      "Conv2 output shape: torch.Size([5, 512, 67])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  38%|███▊      | 11/29 [00:02<00:04,  3.98it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 187])\n",
      "Conv2 output shape: torch.Size([5, 512, 187])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 123])\n",
      "Conv2 output shape: torch.Size([5, 512, 123])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  41%|████▏     | 12/29 [00:03<00:03,  4.30it/s, loss=18.8, train_ca=4, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 63])\n",
      "Conv2 output shape: torch.Size([5, 512, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  48%|████▊     | 14/29 [00:03<00:04,  3.16it/s, loss=189, train_ca=0, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  52%|█████▏    | 15/29 [00:04<00:03,  3.64it/s, loss=15.9, train_ca=8.5, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 52])\n",
      "Conv2 output shape: torch.Size([5, 512, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  59%|█████▊    | 17/29 [00:04<00:03,  3.60it/s, loss=121, train_ca=4.5, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  62%|██████▏   | 18/29 [00:04<00:02,  3.85it/s, loss=43.5, train_ca=6.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  69%|██████▉   | 20/29 [00:05<00:02,  4.36it/s, loss=26.6, train_ca=5, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 74])\n",
      "Conv2 output shape: torch.Size([5, 512, 74])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  76%|███████▌  | 22/29 [00:05<00:01,  5.54it/s, loss=27.2, train_ca=4, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 64])\n",
      "Conv2 output shape: torch.Size([5, 512, 64])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 88])\n",
      "Conv2 output shape: torch.Size([5, 512, 88])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  79%|███████▉  | 23/29 [00:05<00:01,  5.82it/s, loss=25.5, train_ca=10, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 83])\n",
      "Conv2 output shape: torch.Size([5, 512, 83])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  83%|████████▎ | 24/29 [00:06<00:01,  3.89it/s, loss=119, train_ca=11.7, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training:  93%|█████████▎| 27/29 [00:06<00:00,  5.56it/s, loss=17.1, train_ca=6.5, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 56])\n",
      "Conv2 output shape: torch.Size([5, 512, 56])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [10]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.37it/s, loss=15.5, train_ca=5, train_wa=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 75])\n",
      "Conv2 output shape: torch.Size([5, 512, 75])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 53])\n",
      "Conv2 output shape: torch.Size([2, 512, 53])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.52it/s, val_loss=11.3, val_ca=6, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  5.45it/s, val_loss=33.8, val_ca=6.44, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  62%|██████▎   | 5/8 [00:01<00:00,  6.12it/s, val_loss=15, val_ca=5.47, val_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  5.86it/s, val_loss=14.7, val_ca=10, val_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "37.33695196111997\n",
      "EarlyStopping counter: (-35.358560 6 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:   0%|          | 0/29 [00:00<?, ?it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 67])\n",
      "Conv2 output shape: torch.Size([5, 512, 67])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:   3%|▎         | 1/29 [00:00<00:02,  9.99it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 105])\n",
      "Conv2 output shape: torch.Size([5, 512, 105])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  10%|█         | 3/29 [00:00<00:04,  5.49it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  14%|█▍        | 4/29 [00:01<00:09,  2.63it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 68])\n",
      "Conv2 output shape: torch.Size([5, 512, 68])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  21%|██        | 6/29 [00:01<00:05,  4.47it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 68])\n",
      "Conv2 output shape: torch.Size([5, 512, 68])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  24%|██▍       | 7/29 [00:01<00:06,  3.55it/s, loss=0, train_ca=0, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 61])\n",
      "Conv2 output shape: torch.Size([5, 512, 61])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  34%|███▍      | 10/29 [00:02<00:03,  5.51it/s, loss=24.9, train_ca=10.4, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 70])\n",
      "Conv2 output shape: torch.Size([5, 512, 70])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 76])\n",
      "Conv2 output shape: torch.Size([5, 512, 76])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  41%|████▏     | 12/29 [00:02<00:02,  6.60it/s, loss=20.3, train_ca=8.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 78])\n",
      "Conv2 output shape: torch.Size([5, 512, 78])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 66])\n",
      "Conv2 output shape: torch.Size([5, 512, 66])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  45%|████▍     | 13/29 [00:02<00:02,  6.39it/s, loss=32.3, train_ca=6.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  55%|█████▌    | 16/29 [00:03<00:01,  7.49it/s, loss=14.9, train_ca=12.4, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 50])\n",
      "Conv2 output shape: torch.Size([5, 512, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  55%|█████▌    | 16/29 [00:03<00:01,  7.49it/s, loss=16.5, train_ca=12, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 56])\n",
      "Conv2 output shape: torch.Size([5, 512, 56])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  62%|██████▏   | 18/29 [00:03<00:01,  6.73it/s, loss=46.4, train_ca=4.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 138])\n",
      "Conv2 output shape: torch.Size([5, 512, 138])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  66%|██████▌   | 19/29 [00:03<00:01,  5.43it/s, loss=73.2, train_ca=7.86, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 187])\n",
      "Conv2 output shape: torch.Size([5, 512, 187])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  69%|██████▉   | 20/29 [00:04<00:02,  4.12it/s, loss=108, train_ca=2, train_wa=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  72%|███████▏  | 21/29 [00:04<00:02,  2.74it/s, loss=0, train_ca=0, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  76%|███████▌  | 22/29 [00:04<00:02,  3.21it/s, loss=33, train_ca=6, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  83%|████████▎ | 24/29 [00:05<00:01,  2.96it/s, loss=125, train_ca=2.5, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  86%|████████▌ | 25/29 [00:05<00:01,  3.31it/s, loss=58.9, train_ca=4.5, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training:  93%|█████████▎| 27/29 [00:06<00:00,  4.38it/s, loss=16.7, train_ca=9.43, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 123])\n",
      "Conv2 output shape: torch.Size([5, 512, 123])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 59])\n",
      "Conv2 output shape: torch.Size([5, 512, 59])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.50it/s, loss=27, train_ca=5, train_wa=0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 121])\n",
      "Conv2 output shape: torch.Size([5, 512, 121])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 88])\n",
      "Conv2 output shape: torch.Size([2, 512, 88])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.48it/s, val_loss=11.3, val_ca=2, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  5.37it/s, val_loss=33.7, val_ca=2, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 6/8 [00:01<00:00,  7.12it/s, val_loss=13.1, val_ca=6.22, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  5.97it/s, val_loss=14.5, val_ca=10, val_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "37.46178838839898\n",
      "EarlyStopping counter: (-35.358560 7 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:   0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 79])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:   7%|▋         | 2/29 [00:00<00:04,  6.27it/s, loss=32.1, train_ca=2, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  14%|█▍        | 4/29 [00:00<00:03,  7.13it/s, loss=20.1, train_ca=4, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 105])\n",
      "Conv2 output shape: torch.Size([5, 512, 105])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 67])\n",
      "Conv2 output shape: torch.Size([5, 512, 67])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  17%|█▋        | 5/29 [00:01<00:06,  3.88it/s, loss=21.8, train_ca=10.4, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([5, 512, 71])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  24%|██▍       | 7/29 [00:01<00:04,  4.84it/s, loss=33.2, train_ca=8.5, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 121])\n",
      "Conv2 output shape: torch.Size([5, 512, 121])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  31%|███       | 9/29 [00:01<00:04,  4.86it/s, loss=43.7, train_ca=4.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  34%|███▍      | 10/29 [00:02<00:07,  2.71it/s, loss=190, train_ca=2.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  38%|███▊      | 11/29 [00:03<00:08,  2.08it/s, loss=178, train_ca=8.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  41%|████▏     | 12/29 [00:03<00:06,  2.67it/s, loss=36.5, train_ca=11.7, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 104])\n",
      "Conv2 output shape: torch.Size([5, 512, 104])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  48%|████▊     | 14/29 [00:03<00:04,  3.72it/s, loss=20.1, train_ca=7, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 64])\n",
      "Conv2 output shape: torch.Size([5, 512, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  55%|█████▌    | 16/29 [00:03<00:02,  5.03it/s, loss=32.6, train_ca=4, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 53])\n",
      "Conv2 output shape: torch.Size([5, 512, 53])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  59%|█████▊    | 17/29 [00:04<00:02,  5.24it/s, loss=32.4, train_ca=6.44, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 100])\n",
      "Conv2 output shape: torch.Size([5, 512, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  62%|██████▏   | 18/29 [00:04<00:02,  3.80it/s, loss=104, train_ca=4.22, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  66%|██████▌   | 19/29 [00:04<00:02,  4.19it/s, loss=36.5, train_ca=5.43, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 123])\n",
      "Conv2 output shape: torch.Size([5, 512, 123])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  72%|███████▏  | 21/29 [00:05<00:01,  4.84it/s, loss=16.3, train_ca=2, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 187])\n",
      "Conv2 output shape: torch.Size([5, 512, 187])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 53])\n",
      "Conv2 output shape: torch.Size([5, 512, 53])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  79%|███████▉  | 23/29 [00:05<00:01,  5.38it/s, loss=18.6, train_ca=4, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 63])\n",
      "Conv2 output shape: torch.Size([5, 512, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  83%|████████▎ | 24/29 [00:05<00:00,  5.26it/s, loss=16.9, train_ca=0, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 138])\n",
      "Conv2 output shape: torch.Size([5, 512, 138])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 55])\n",
      "Conv2 output shape: torch.Size([5, 512, 55])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training:  93%|█████████▎| 27/29 [00:06<00:00,  7.09it/s, loss=15.2, train_ca=10.4, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 63])\n",
      "Conv2 output shape: torch.Size([5, 512, 63])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 50])\n",
      "Conv2 output shape: torch.Size([5, 512, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [12]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.43it/s, loss=19.8, train_ca=17.4, train_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 56])\n",
      "Conv2 output shape: torch.Size([2, 512, 56])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.41it/s, val_loss=11.3, val_ca=6, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  5.15it/s, val_loss=33.8, val_ca=6.44, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 6/8 [00:01<00:00,  6.64it/s, val_loss=15, val_ca=5.47, val_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  5.77it/s, val_loss=14.7, val_ca=10, val_wa=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "37.60446092060634\n",
      "EarlyStopping counter: (-35.358560 8 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:   3%|▎         | 1/29 [00:00<00:02,  9.71it/s, loss=19.7, train_ca=16.9, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 63])\n",
      "Conv2 output shape: torch.Size([5, 512, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:   7%|▋         | 2/29 [00:00<00:02,  9.55it/s, loss=18.4, train_ca=6, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 59])\n",
      "Conv2 output shape: torch.Size([5, 512, 59])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  10%|█         | 3/29 [00:00<00:04,  5.84it/s, loss=57.5, train_ca=6.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 173])\n",
      "Conv2 output shape: torch.Size([5, 512, 173])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  17%|█▋        | 5/29 [00:00<00:03,  6.04it/s, loss=25.8, train_ca=14.9, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 93])\n",
      "Conv2 output shape: torch.Size([5, 512, 93])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 70])\n",
      "Conv2 output shape: torch.Size([5, 512, 70])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  21%|██        | 6/29 [00:00<00:03,  6.73it/s, loss=19.9, train_ca=10.4, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 64])\n",
      "Conv2 output shape: torch.Size([5, 512, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  24%|██▍       | 7/29 [00:01<00:05,  3.80it/s, loss=25.3, train_ca=10.5, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 344])\n",
      "Conv2 output shape: torch.Size([5, 512, 344])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  31%|███       | 9/29 [00:01<00:03,  5.15it/s, loss=23.5, train_ca=2.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 79])\n",
      "Conv2 output shape: torch.Size([5, 512, 79])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 115])\n",
      "Conv2 output shape: torch.Size([5, 512, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  41%|████▏     | 12/29 [00:02<00:02,  6.93it/s, loss=14.5, train_ca=12, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([5, 512, 71])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 50])\n",
      "Conv2 output shape: torch.Size([5, 512, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  45%|████▍     | 13/29 [00:02<00:02,  5.99it/s, loss=36.4, train_ca=12.6, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 172])\n",
      "Conv2 output shape: torch.Size([5, 512, 172])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 102])\n",
      "Conv2 output shape: torch.Size([5, 512, 102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  52%|█████▏    | 15/29 [00:02<00:02,  5.84it/s, loss=37.7, train_ca=3.43, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 123])\n",
      "Conv2 output shape: torch.Size([5, 512, 123])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  55%|█████▌    | 16/29 [00:03<00:03,  4.09it/s, loss=107, train_ca=2.22, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 298])\n",
      "Conv2 output shape: torch.Size([5, 512, 298])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 101])\n",
      "Conv2 output shape: torch.Size([5, 512, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  62%|██████▏   | 18/29 [00:03<00:02,  4.61it/s, loss=43.5, train_ca=4, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 133])\n",
      "Conv2 output shape: torch.Size([5, 512, 133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  69%|██████▉   | 20/29 [00:03<00:01,  5.12it/s, loss=20.9, train_ca=12, train_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 165])\n",
      "Conv2 output shape: torch.Size([5, 512, 165])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 68])\n",
      "Conv2 output shape: torch.Size([5, 512, 68])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  72%|███████▏  | 21/29 [00:04<00:01,  5.59it/s, loss=28.6, train_ca=10.7, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 105])\n",
      "Conv2 output shape: torch.Size([5, 512, 105])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 89])\n",
      "Conv2 output shape: torch.Size([5, 512, 89])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  79%|███████▉  | 23/29 [00:04<00:00,  6.53it/s, loss=15.2, train_ca=2, train_wa=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 67])\n",
      "Conv2 output shape: torch.Size([5, 512, 67])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 52])\n",
      "Conv2 output shape: torch.Size([5, 512, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  86%|████████▌ | 25/29 [00:05<00:01,  3.65it/s, loss=225, train_ca=2.22, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 532])\n",
      "Conv2 output shape: torch.Size([5, 512, 532])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  90%|████████▉ | 26/29 [00:05<00:00,  3.11it/s, loss=114, train_ca=4.5, train_wa=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 326])\n",
      "Conv2 output shape: torch.Size([5, 512, 326])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training:  93%|█████████▎| 27/29 [00:05<00:00,  3.16it/s, loss=64.6, train_ca=4.5, train_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 187])\n",
      "Conv2 output shape: torch.Size([5, 512, 187])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [13]/[15] Training: 100%|██████████| 29/29 [00:06<00:00,  4.42it/s, loss=21.5, train_ca=0, train_wa=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 511])\n",
      "Conv2 output shape: torch.Size([5, 512, 511])\n",
      "Conv1 output shape: torch.Size([2, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([2, 512, 71])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  12%|█▎        | 1/8 [00:00<00:02,  2.37it/s, val_loss=11.3, val_ca=4, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 290])\n",
      "Conv2 output shape: torch.Size([5, 512, 290])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 39])\n",
      "Conv2 output shape: torch.Size([5, 512, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  38%|███▊      | 3/8 [00:00<00:01,  4.85it/s, val_loss=33.8, val_ca=6.44, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 103])\n",
      "Conv2 output shape: torch.Size([5, 512, 103])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 141])\n",
      "Conv2 output shape: torch.Size([5, 512, 141])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  75%|███████▌  | 6/8 [00:01<00:00,  6.88it/s, val_loss=14.1, val_ca=6.22, val_wa=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 60])\n",
      "Conv2 output shape: torch.Size([5, 512, 60])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 54])\n",
      "Conv2 output shape: torch.Size([5, 512, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 8/8 [00:01<00:00,  5.82it/s, val_loss=14.5, val_ca=10, val_wa=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 204])\n",
      "Conv2 output shape: torch.Size([5, 512, 204])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 51])\n",
      "Conv2 output shape: torch.Size([1, 512, 51])\n",
      "37.712663833300276\n",
      "EarlyStopping counter: (-35.358560 9 out of 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#The alphabet was altered to characters that were being used on the Car Plates.\n",
    "# When training the model with the inicial parameters from the original code no predictions would be made. In order to get the predictions, and also improve the results, \n",
    "# the number of hidden layers were decreased to 128, andthe batch size to 5. The learning_rate went up to 0.01 and I also increased the number of epochs 15. \n",
    "\n",
    "\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "args = {\n",
    "    'name':'exp1',\n",
    "    'path':'Car/LP-characters/images',\n",
    "    'imgdir': 'train',\n",
    "    'imgH':32, #tvz precisa mexer\n",
    "    'nChannels':1, #gray\n",
    "    'nHidden':128,\n",
    "    'nClasses':len(alphabet),\n",
    "    'lr':0.01,\n",
    "    'epochs':15,\n",
    "    'batch_size':5,\n",
    "    'save_dir':'checkpoints',\n",
    "    'log_dir':'logs',\n",
    "    'resume':False,\n",
    "    'cuda':False,\n",
    "    'schedule':False\n",
    "}\n",
    "\n",
    "data = SynthDataset(args)\n",
    "args['collate_fn'] = SynthCollator()\n",
    "train_split = int(0.8*len(data))\n",
    "val_split = len(data) - train_split\n",
    "\n",
    "args['data_train'], args['data_val'] = random_split(data, (train_split, val_split))\n",
    "\n",
    "print('Traininig Data Size:{}\\nVal Data Size:{}'.format(\n",
    "    len(args['data_train']), len(args['data_val'])))\n",
    "args['alphabet'] = alphabet\n",
    "model = CRNN(args)\n",
    "args['criterion'] = CustomCTCLoss()\n",
    "savepath = os.path.join(args['save_dir'], args['name'])\n",
    "gmkdir(savepath)\n",
    "gmkdir(args['log_dir'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "learner = Learner(model, optimizer, savepath=savepath, resume=args['resume'])\n",
    "learner.fit(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e677efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bafd5d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_accuracy(args):\n",
    "    loader = torch.utils.data.DataLoader(args['data'],\n",
    "                batch_size=args['batch_size'],\n",
    "                collate_fn=args['collate_fn'])\n",
    "    model = args['model']\n",
    "    model.eval()\n",
    "    converter = OCRLabelConverter(args['alphabet'])\n",
    "    evaluator = Eval()\n",
    "    labels, predictions, images = [], [], []\n",
    "    for iteration, batch in enumerate(tqdm(loader)):\n",
    "        input_, targets = batch['img'].to(device), batch['label']\n",
    "        images.extend(input_.squeeze().detach())\n",
    "        labels.extend(targets)\n",
    "        targets, lengths = converter.encode(targets)\n",
    "        logits = model(input_).transpose(1, 0)\n",
    "        logits = torch.nn.functional.log_softmax(logits, 2)\n",
    "        logits = logits.contiguous().cpu()\n",
    "        T, B, H = logits.size()\n",
    "        pred_sizes = torch.LongTensor([T for i in range(B)])\n",
    "        probs, pos = logits.max(2)\n",
    "        pos = pos.transpose(1, 0).contiguous().view(-1)\n",
    "        sim_preds = converter.decode(pos.data, pred_sizes.data, raw=False)\n",
    "        predictions.extend(sim_preds)\n",
    "#     make_grid(images[:10], nrow=2)\n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    columns = 4\n",
    "    rows = 5\n",
    "    for i in range(1, columns*rows +1):\n",
    "        img = images[i].cpu()\n",
    "        img = (img - img.min())/(img.max() - img.min())\n",
    "        img = np.array(img * 255.0, dtype=np.uint8)\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.title(predictions[i])\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n",
    "    ca = np.mean((list(map(evaluator.char_accuracy, list(zip(predictions, labels))))))\n",
    "    wa = np.mean((list(map(evaluator.word_accuracy_line, list(zip(predictions, labels))))))\n",
    "    return ca, wa,predictions,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c40d5596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoints\\exp1\\best.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 122])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:00<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conv2 output shape: torch.Size([5, 512, 122])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 71])\n",
      "Conv2 output shape: torch.Size([5, 512, 71])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:00<00:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 120])\n",
      "Conv2 output shape: torch.Size([5, 512, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:00<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 120])\n",
      "Conv2 output shape: torch.Size([5, 512, 120])\n",
      "Conv1 output shape: torch.Size([5, 512, 1, 73])\n",
      "Conv2 output shape: torch.Size([5, 512, 73])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 output shape: torch.Size([5, 512, 1, 69])\n",
      "Conv2 output shape: torch.Size([5, 512, 69])\n",
      "Conv1 output shape: torch.Size([1, 512, 1, 34])\n",
      "Conv2 output shape: torch.Size([1, 512, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Accuracy: 11.06\n",
      "Word Accuracy: 0.00\n",
      "['6KD6LZ6', '6KD6LZ6', '6KD6LZ6', '6KD6LZ6', '6KD6LZ6', 'KLBEKI', 'KLBEKI', 'KLBEKI', 'KLBEKI', 'KLBEKI', 'KLKLBEI', 'KLKLBEI', 'KLKLBEI', 'KLKLBEI', 'KLKLBEI', 'KLKLBI', 'KLKLBI', 'KLKLBI', 'KLKLBI', 'KLKLBI', 'KEKEBZK', 'KEKEBZK', 'KEKEBZK', 'KEKEBZK', 'KEKEBZK', 'KEKEBZ', 'KEKEBZ', 'KEKEBZ', 'KEKEBZ', 'KEKEBZ', 'Q', 'K', 'L', 'Q', '6', 'L', 'E', 'Q']\n",
      "['46Z8892MH', 'AK5600MH12', 'AP20N3100', 'DL10CG4693', 'DL49AK49', 'GJ05443', 'GJ7BB7666', 'H20CS1938', 'HR26BA8008', 'HR26BP3543', 'HR26DG6167', 'KA04ME9869', 'KL01BR8055', 'KL01CC50', 'KL07BF5000', 'KL09AL9540', 'KL10AV6342', 'KL43B2344', 'KL65E1000', 'MH02AJ344', 'MH02CB4545', 'MH15BD8877', 'MH15TC554', 'MH20CS1941', 'MK20TC189B', 'TN07BV5200', 'TN42R2697', 'TN43J0158', 'TN59AQ1515', 'TS09EB1458', 'WOBNP300']\n"
     ]
    }
   ],
   "source": [
    "args['imgdir'] = 'test'\n",
    "args['data'] = SynthDataset(args)\n",
    "resume_file = os.path.join(args['save_dir'], args['name'], 'best.ckpt')\n",
    "if os.path.isfile(resume_file):\n",
    "    print('Loading model %s'%resume_file)\n",
    "    #checkpoint = torch.load(resume_file)\n",
    "    #model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    args['model'] = model\n",
    "    ca, wa,pred,labels = get_accuracy(args)\n",
    "    print(\"Character Accuracy: %.2f\\nWord Accuracy: %.2f\"%(ca, wa))\n",
    "    print(pred)\n",
    "    print(labels)\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(save_file))\n",
    "    print('Exiting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6daeefb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch.version' from 'c:\\\\Python311\\\\Lib\\\\site-packages\\\\torch\\\\version.py'>\n"
     ]
    }
   ],
   "source": [
    "print(torch.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0886f28",
   "metadata": {},
   "source": [
    "## Part 4 and 5\n",
    "\n",
    "\n",
    "The model's performance has been challenging, but I think it is expected due to the size of our dataset. So, as expected, the accuracy is relatively low due to the limited amount of training data. Although the accuracy has changed with each run, it hasn't exceeded 11%. Initially, the model struggled to make any predictions, with accuracy being either 0 or less than 1%. After implementing some changes, such as increasing the number of hidden layers and adjusting the batch size, I was able to achieve 1-5% accuracy. Moving forward, when I increased the number of epochs, the learning rate, and changed max pooling to average pooling, I managed to boost the accuracy to 11%. Even though I had expected a low accuracy, it the performance was poor, but I couldn't make it better.\n",
    "\n",
    "To improve the accuracy and enhance performance, we could try increasing the dataset size to make it at least 10 times bigger, so the model has enough information to be trained on. The quality of the dataset could also be improved since some of the pictures are challenging even for human eyes. One of the possible mistakes might be related to character recognition, so I believe we could also try pre-trained models to minimize it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
